{"cells":[{"cell_type":"markdown","source":["#Use CNTK on a single node to train a handwritten digit recognition model\nThis notebook demonstrates the use of the CNTK package to train a feed-forward convolutional network to recognize digits from the MNIST database. \n\nThe contents of this notebook is based on CNTK tutorial [103A](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103A_MNIST_DataLoader.ipynb) and [103B](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_103B_MNIST_FeedForwardNetwork.ipynb) with minor modifications to run on Databricks. Thanks to the developers of CNTK for this tutorial!"],"metadata":{}},{"cell_type":"markdown","source":["# Feedforward network for MNIST\nThis tutorial is targeted to individuals who are new to CNTK and to machine learning. We assume you have completed or are familiar with [CNTK 101](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_101_LogisticRegression.ipynb) and [102](https://github.com/Microsoft/CNTK/blob/master/Tutorials/CNTK_102_FeedForward.ipynb). In this tutorial, you will train a feed forward network based simple model to recognize handwritten digits. This is the first example, where we will train and evaluate a neural network based model on read real world data.\nCNTK 103 tutorial is divided into two parts:\n\n* Part A: Familiarize with the MNIST database that will be used later in the tutorial\n* Part B: We will use the feedforward classifier used in CNTK 102 to classify digits in MNIST data set"],"metadata":{}},{"cell_type":"markdown","source":["Import relevant modules to be used later"],"metadata":{}},{"cell_type":"code","source":["from __future__ import print_function\nimport gzip\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport shutil\nimport struct\nimport sys\n\ntry: \n    from urllib.request import urlretrieve \nexcept ImportError: \n    from urllib import urlretrieve"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":["## Part A: Data download\nWe will download the data into local machine. The MNIST database is a standard handwritten digits that has been widely used for training and testing of machine learning algorithms. It has a training set of 60,000 images and a test set of 10,000 images with each image being 28 x 28 pixels. This set is easy to use visualize and train on any computer."],"metadata":{}},{"cell_type":"code","source":["# Functions to load MNIST images and unpack into train and test set.\n# - loadData reads image data and formats into a 28x28 long array\n# - loadLabels reads the corresponding labels data, 1 for each image\n# - load packs the downloaded image and labels data into a combined format to be read later by \n#   CNTK text reader \n\ndef loadData(src, cimg):\n    print ('Downloading ' + src)\n    gzfname, h = urlretrieve(src, './delete.me')\n    print ('Done.')\n    try:\n        with gzip.open(gzfname) as gz:\n            n = struct.unpack('I', gz.read(4))\n            # Read magic number.\n            if n[0] != 0x3080000:\n                raise Exception('Invalid file: unexpected magic number.')\n            # Read number of entries.\n            n = struct.unpack('>I', gz.read(4))[0]\n            if n != cimg:\n                raise Exception('Invalid file: expected {0} entries.'.format(cimg))\n            crow = struct.unpack('>I', gz.read(4))[0]\n            ccol = struct.unpack('>I', gz.read(4))[0]\n            if crow != 28 or ccol != 28:\n                raise Exception('Invalid file: expected 28 rows/cols per image.')\n            # Read data.\n            res = np.fromstring(gz.read(cimg * crow * ccol), dtype = np.uint8)\n    finally:\n        os.remove(gzfname)\n    return res.reshape((cimg, crow * ccol))\n\ndef loadLabels(src, cimg):\n    print ('Downloading ' + src)\n    gzfname, h = urlretrieve(src, './delete.me')\n    print ('Done.')\n    try:\n        with gzip.open(gzfname) as gz:\n            n = struct.unpack('I', gz.read(4))\n            # Read magic number.\n            if n[0] != 0x1080000:\n                raise Exception('Invalid file: unexpected magic number.')\n            # Read number of entries.\n            n = struct.unpack('>I', gz.read(4))\n            if n[0] != cimg:\n                raise Exception('Invalid file: expected {0} rows.'.format(cimg))\n            # Read labels.\n            res = np.fromstring(gz.read(cimg), dtype = np.uint8)\n    finally:\n        os.remove(gzfname)\n    return res.reshape((cimg, 1))\n\ndef try_download(dataSrc, labelsSrc, cimg):\n    data = loadData(dataSrc, cimg)\n    labels = loadLabels(labelsSrc, cimg)\n    return np.hstack((data, labels))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## Download the data\nThe MNIST data is provided as train and test set. Training set has 60000 images while the test set has 10000 images. Let us download the data."],"metadata":{}},{"cell_type":"code","source":["# URLs for the train image and labels data\nurl_train_image = 'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz'\nurl_train_labels = 'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz'\nnum_train_samples = 60000\n\nprint(\"Downloading train data\")\ntrain = try_download(url_train_image, url_train_labels, num_train_samples)\n\n\nurl_test_image = 'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz'\nurl_test_labels = 'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz'\nnum_test_samples = 10000\n\nprint(\"Downloading test data\")\ntest = try_download(url_test_image, url_test_labels, num_test_samples)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["## Visualize the data"],"metadata":{}},{"cell_type":"code","source":["for i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(train[i, :-1].reshape(28,28), cmap='Greys_r')\n    plt.title(str(train[i,-1]),fontsize=20, fontweight=\"bold\", color=\"black\")\n    plt.axis('off')\nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["## Save the images\nSave the images in a local directory. While saving the data we flatten the images to a vector (28x28 image pixels becomes an array of length 784 data points) and the labels are encoded as [1-hot](https://en.wikipedia.org/wiki/One-hot) encoding (label of 3 with 10 digits becomes 0010000000."],"metadata":{}},{"cell_type":"code","source":["# Save the data files into a format compatible with CNTK text reader\ndef savetxt(filename, ndarray):\n    dir = os.path.dirname(filename)\n\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n\n    if not os.path.isfile(filename):\n        print(\"Saving\", filename )\n        with open(filename, 'w') as f:\n            labels = list(map(' '.join, np.eye(10, dtype=np.uint).astype(str)))\n            for row in ndarray:\n                row_str = row.astype(str)\n                label_str = labels[row[-1]]\n                feature_str = ' '.join(row_str[:-1])\n                f.write('|labels {} |features {}\\n'.format(label_str, feature_str))\n    else:\n        print(\"File already exists\", filename)"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"code","source":["# Save the train and test files (prefer our default path for the data)\ndata_dir = os.path.join(\"..\", \"Examples\", \"Image\", \"DataSets\", \"MNIST\")\nif not os.path.exists(data_dir):\n    data_dir = os.path.join(\"data\", \"MNIST\")\n\nprint ('Writing train text file...')\nsavetxt(os.path.join(data_dir, \"Train-28x28_cntk_text.txt\"), train)\n\nprint ('Writing test text file...')\nsavetxt(os.path.join(data_dir, \"Test-28x28_cntk_text.txt\"), test)\n\nprint('Done')"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":["##Part B: Training a feedforward convolutional network"],"metadata":{}},{"cell_type":"code","source":["import cntk as C\nfrom cntk import Trainer, learning_rate_schedule, UnitType\nfrom cntk.io import CTFDeserializer, MinibatchSource, StreamDef, StreamDefs\nfrom cntk.io import INFINITELY_REPEAT, FULL_DATA_SWEEP\nfrom cntk.initializer import glorot_uniform\nfrom cntk.layers import default_options, Input, Dense\nfrom cntk.learner import sgd\n\n# Select the right target device when this notebook is being tested:\nif 'TEST_DEVICE' in os.environ:\n    import cntk\n    if os.environ['TEST_DEVICE'] == 'cpu':\n        cntk.device.set_default_device(cntk.device.cpu())\n    else:\n        cntk.device.set_default_device(cntk.device.gpu(0))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["# Ensure we always get the same amount of randomness\nnp.random.seed(0)\n\n# Define the data dimensions\ninput_dim = 784\nnum_output_classes = 10"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["## Data reading\n\nWe define a ``create_reader`` function to read the training and test data using the [CTF deserializer](https://cntk.ai/pythondocs/cntk.io.html?highlight=ctfdeserializer#cntk.io.CTFDeserializer). The labels are 1-hot encoded."],"metadata":{}},{"cell_type":"code","source":["# Read a CTF formatted text (as mentioned above) using the CTF deserializer from a file\ndef create_reader(path, is_training, input_dim, num_label_classes):\n    return MinibatchSource(CTFDeserializer(path, StreamDefs(\n        labels = StreamDef(field='labels', shape=num_label_classes, is_sparse=False),\n        features   = StreamDef(field='features', shape=input_dim, is_sparse=False)\n    )), randomize = is_training, epoch_size = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["## Model Creation\nOur feed forward network will be relatively simple with 2 hidden layers (``num_hidden_layers``) with each layer having 200 hidden nodes (``hidden_layers_dim``).\n\n<img src=\"http://cntk.ai/jup/feedforward_network.jpg\" width=400 height=100>\n\nIf you are not familiar with the terms \"hidden layer\" and \"number of hidden layers\", please refer back to CNTK 102 tutorial.\nFor this tutorial: The number of green nodes (refer to picture above) in each hidden layer is set to 200 and the number of hidden layers (refer to the number of layers of green nodes) is 2. Fill in the following values:\n\n* ``num_hidden_layers``\n* ``hidden_layers_dim``\n\nNote: In this illustration, we have not shown the bias node (introduced in the logistic regression tutorial). Each hidden layer would have a bias node."],"metadata":{}},{"cell_type":"code","source":["num_hidden_layers = 2\nhidden_layers_dim = 400"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["Network input and output: \n- **input** variable (a key CNTK concept): \n>An **input** variable is a container in which we fill different observations in this case image pixels during model learning (a.k.a.training) and model evaluation (a.k.a. testing). Thus, the shape of the `input_variable` must match the shape of the data that will be provided.  For example, when data are images each of  height 10 pixels  and width 5 pixels, the input feature dimension will be 50 (representing the total number of image pixels). More on data and their dimensions to appear in separate tutorials.\n\n\n**Question** What is the input dimension of your chosen model? This is fundamental to our understanding of variables in a network or model representation in CNTK."],"metadata":{}},{"cell_type":"code","source":["input = Input(input_dim)\nlabel = Input(num_output_classes)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["## Feed forward network setup\n\nIf you are not familiar with the feedforward network, please refer to CNTK 102. In this tutorial we are using the same network."],"metadata":{}},{"cell_type":"code","source":["def create_model(features):\n    with default_options(init = glorot_uniform(), activation = C.ops.relu):\n            h = features\n            for _ in range(num_hidden_layers):\n                h = Dense(hidden_layers_dim)(h)\n            r = Dense(num_output_classes, activation = None)(h)\n            return r\n        \nz = create_model(input)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["`z` will be used to represent the output of a network.\n\nWe introduced sigmoid function in CNTK 102, in this tutorial you should try different activation functions. You may choose to do this right away and take a peek into the performance later in the tutorial or run the preset tutorial and then choose to perform the suggested activity.\n\n\n** Suggested Activity **\n- Record the training error you get with `sigmoid` as the activation function\n- Now change to `relu` as the activation function and see if you can improve your training error\n\n*Quiz*: Different supported activation functions can be [found here][]. Which activation function gives the least training error?\n\n[found here]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions"],"metadata":{}},{"cell_type":"code","source":["# Scale the input to 0-1 range by dividing each pixel by 256.\nz = create_model(input/256.0)"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Learning model parameters\n\nSame as the previous tutorial, we use the `softmax` function to map the accumulated evidences or activations to a probability distribution over the classes (Details of the [softmax function][] and other [activation][] functions).\n\n[softmax function]: http://cntk.ai/pythondocs/cntk.ops.html#cntk.ops.softmax\n\n[activation]: https://github.com/Microsoft/CNTK/wiki/Activation-Functions"],"metadata":{}},{"cell_type":"markdown","source":["## Training\n\nSimilar to CNTK 102, we use minimize the cross-entropy between the label and predicted probability by the network. If this terminology sounds strange to you, please refer to the CNTK 102 for a refresher."],"metadata":{}},{"cell_type":"code","source":["loss = C.ops.cross_entropy_with_softmax(z, label)"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["#### Evaluation\n\nIn order to evaluate the classification, one can compare the output of the network which for each observation emits a vector of evidences (can be converted into probabilities using `softmax` functions) with dimension equal to number of classes."],"metadata":{}},{"cell_type":"code","source":["label_error = C.ops.classification_error(z, label)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["### Configure training\n\nThe trainer strives to reduce the `loss` function by different optimization approaches, [Stochastic Gradient Descent][] (`sgd`) being one of the most popular one. Typically, one would start with random initialization of the model parameters. The `sgd` optimizer would calculate the `loss` or error between the predicted label against the corresponding ground-truth label and using [gradient-decent][] generate a new set model parameters in a single iteration. \n\nThe aforementioned model parameter update using a single observation at a time is attractive since it does not require the entire data set (all observation) to be loaded in memory and also requires gradient computation over fewer datapoints, thus allowing for training on large data sets. However, the updates generated using a single observation sample at a time can vary wildly between iterations. An intermediate ground is to load a small set of observations and use an average of the `loss` or error from that set to update the model parameters. This subset is called a *minibatch*.\n\nWith minibatches we often sample observation from the larger training dataset. We repeat the process of model parameters update using different combination of training samples and over a period of time minimize the `loss` (and the error). When the incremental error rates are no longer changing significantly or after a preset number of maximum minibatches to train, we claim that our model is trained.\n\nOne of the key parameter for optimization is called the `learning_rate`. For now, we can think of it as a scaling factor that modulates how much we change the parameters in any iteration. We will be covering more details in later tutorial. \nWith this information, we are ready to create our trainer. \n\n[optimization]: https://en.wikipedia.org/wiki/Category:Convex_optimization\n[Stochastic Gradient Descent]: https://en.wikipedia.org/wiki/Stochastic_gradient_descent\n[gradient-decent]: http://www.statisticsviews.com/details/feature/5722691/Getting-to-the-Bottom-of-Regression-with-Gradient-Descent.html"],"metadata":{}},{"cell_type":"code","source":["# Instantiate the trainer object to drive the model training\nlearning_rate = 0.2\nlr_schedule = learning_rate_schedule(learning_rate, UnitType.minibatch)\nlearner = sgd(z.parameters, lr_schedule)\ntrainer = Trainer(z, (loss, label_error), [learner])"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["First let us create some helper functions that will be needed to visualize different functions associated with training."],"metadata":{}},{"cell_type":"code","source":["from cntk.utils import get_train_eval_criterion, get_train_loss\n\n# Define a utility function to compute the moving average sum.\n# A more efficient implementation is possible with np.cumsum() function\ndef moving_average(a, w=5):\n    if len(a) < w:\n        return a[:]    # Need to send a copy of the array\n    return [val if idx < w else sum(a[(idx-w):idx])/w for idx, val in enumerate(a)]\n\n\n# Defines a utility that prints the training progress\ndef print_training_progress(trainer, mb, frequency, verbose=1):\n    training_loss = \"NA\"\n    eval_error = \"NA\"\n\n    if mb%frequency == 0:\n        training_loss = get_train_loss(trainer)\n        eval_error = get_train_eval_criterion(trainer)\n        if verbose: \n            print (\"Minibatch: {0:6d}, Loss: {1:.4f}, Error: {2:.2f}%\".format(mb, training_loss, eval_error*100))\n        \n    return mb, training_loss, eval_error"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["<a id='#Run the trainer'></a>\n### Run the trainer\n\nWe are now ready to train our fully connected neural net. We want to decide what data we need to feed into the training engine.\n\nIn this example, each iteration of the optimizer will work on `minibatch_size` sized samples. We would like to train on all 60000 observations. Additionally we will make multiple passes through the data specified by the variable `num_sweeps_to_train_with`. With these parameters we can proceed with training our simple feed forward network."],"metadata":{}},{"cell_type":"code","source":["# Initialize the parameters for the trainer\nminibatch_size = 64\nnum_samples_per_sweep = 60000\nnum_sweeps_to_train_with = 20\nnum_minibatches_to_train = (num_samples_per_sweep * num_sweeps_to_train_with) / minibatch_size"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["# Create the reader to training data set\ntrain_file = os.path.join(data_dir, \"Train-28x28_cntk_text.txt\")\nreader_train = create_reader(train_file, True, input_dim, num_output_classes)\n\n# Map the data streams to the input and labels.\ninput_map = {\n    label  : reader_train.streams.labels,\n    input  : reader_train.streams.features\n} \n\n# Run the trainer on and perform model training\ntraining_progress_output_freq = 500\n\nplotdata = {\"batchsize\":[], \"loss\":[], \"error\":[]}\n\nfor i in range(0, int(num_minibatches_to_train)):\n    \n    # Read a mini batch from the training data file\n    data = reader_train.next_minibatch(minibatch_size, input_map = input_map)\n    \n    trainer.train_minibatch(data)\n    batchsize, loss, error = print_training_progress(trainer, i, training_progress_output_freq, verbose=1)\n    \n    if not (loss == \"NA\" or error ==\"NA\"):\n        plotdata[\"batchsize\"].append(batchsize)\n        plotdata[\"loss\"].append(loss)\n        plotdata[\"error\"].append(error)"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Let us plot the errors over the different training minibatches. Note that as we iterate the training loss decreases though we do see some intermediate bumps. \n\nHence, we use smaller minibatches and using `sgd` enables us to have a great scalability while being performant for large data sets. There are advanced variants of the optimizer unique to CNTK that enable harnessing computational efficiency for real world data sets and will be introduced in advanced tutorials."],"metadata":{}},{"cell_type":"code","source":["# Compute the moving average loss to smooth out the noise in SGD\nplotdata[\"avgloss\"] = moving_average(plotdata[\"loss\"])\nplotdata[\"avgerror\"] = moving_average(plotdata[\"error\"])\n\n# Plot the training loss and the training error\nplt.figure()\nplt.subplot(211)\nplt.plot(plotdata[\"batchsize\"], plotdata[\"avgloss\"], 'b--', linewidth=2)\nplt.xlabel('Minibatch number')\nplt.ylabel('Loss')\nplt.title('Minibatch run vs. Training loss')\nplt.grid(\"on\")\nplt.subplot(212)\nplt.plot(plotdata[\"batchsize\"], plotdata[\"avgerror\"], 'r--', linewidth=2)\nplt.xlabel('Minibatch number')\nplt.ylabel('Label Prediction Error')\nplt.title('Minibatch run vs. Label Prediction Error')\nplt.grid(\"on\")\nplt.tight_layout();\nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["## Evaluation / Testing \n\nNow that we have trained the network, let us evaluate the trained network on the test data. This is done using `trainer.test_minibatch`."],"metadata":{}},{"cell_type":"code","source":["# Read the training data\ntest_file = os.path.join(data_dir, \"Test-28x28_cntk_text.txt\")\nreader_test = create_reader(test_file, False, input_dim, num_output_classes)\n\ntest_input_map = {\n    label  : reader_test.streams.labels,\n    input  : reader_test.streams.features,\n}\n\n# Test data for trained model\ntest_minibatch_size = 512\nnum_samples = 10000\nnum_minibatches_to_test = num_samples // test_minibatch_size\ntest_result = 0.0\n\nfor i in range(num_minibatches_to_test):\n    \n    # We are loading test data in batches specified by test_minibatch_size\n    # Each data point in the minibatch is a MNIST digit image of 784 dimensions \n    # with one pixel per dimension that we will encode / decode with the \n    # trained model.\n    data = reader_test.next_minibatch(test_minibatch_size,\n                                      input_map = test_input_map)\n\n    eval_error = trainer.test_minibatch(data)\n    test_result = test_result + eval_error\n\n# Average of evaluation errors of all test minibatches\nprint(\"Average test error: {0:.2f}%\".format(test_result*100 / num_minibatches_to_test))"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["Note, this error is very comparable to our training error indicating that our model has good \"out of sample\" error a.k.a. generalization error. This implies that our model can very effectively deal with previously unseen observations (during the training process). This is key to avoid the phenomenon of overfitting."],"metadata":{}},{"cell_type":"markdown","source":["We have so far been dealing with aggregate measures of error. Let us now get the probabilities associated with individual data points. For each observation, the `eval` function returns the probability distribution across all the classes. The classifier is trained to recognize digits, hence has 10 classes. First let us route the network output through a `softmax` function. This maps the aggregated activations across the network to probabilities across the 10 classes."],"metadata":{}},{"cell_type":"code","source":["out = C.ops.softmax(z)"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["Let us a small minibatch sample from the test data."],"metadata":{}},{"cell_type":"code","source":["# Read the data for evaluation\nreader_eval = create_reader(test_file, False, input_dim, num_output_classes)\n\neval_minibatch_size = 25\neval_input_map = { input  : reader_eval.streams.features } \n\ndata = reader_test.next_minibatch(eval_minibatch_size, input_map = test_input_map)\n\nimg_label = data[label].value\nimg_data = data[input].value\npredicted_label_prob = [out.eval(img_data[i,:,:]) for i in range(img_data.shape[0])]"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"code","source":["# Find the index with the maximum value for both predicted as well as the ground truth\npred = [np.argmax(predicted_label_prob[i]) for i in range(len(predicted_label_prob))]\ngtlabel = [np.argmax(img_label[i,:,:]) for i in range(img_label.shape[0])]"],"metadata":{},"outputs":[],"execution_count":48},{"cell_type":"code","source":["print(\"Label    :\", gtlabel[:25])\nprint(\"Predicted:\", pred)"],"metadata":{},"outputs":[],"execution_count":49},{"cell_type":"code","source":["for i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(img_data[i, :, :].reshape(28,28), cmap='Greys_r')\n    plt.title(str(pred[i]), fontsize=20, fontweight=\"bold\", color=\"green\" if pred[i] == gtlabel[i] else \"red\")    \nplt.show()\ndisplay()"],"metadata":{},"outputs":[],"execution_count":50},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":51}],"metadata":{"name":"CNTK-single-node","notebookId":2499736498219597},"nbformat":4,"nbformat_minor":0}
