{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# How to Quickly Start Competing in the Kaggle Data Science Bowl 2017 with CNTK and LightGBM\n",
    "\n",
    "In this notebook, we share a baseline script that allows anyone interested in participating in the [Data Science Bowl Lung Cancer Detection Competition](https://www.kaggle.com/c/data-science-bowl-2017) to create the first submission quickly. We first extract features from the CT scan images of lungs using a pretrained convolutional neural network (CNN) implemented on [CNTK](https://github.com/Microsoft/CNTK/wiki), followed by running a boosted tree classifier implemented on [LightGBM](https://github.com/Microsoft/LightGBM).  Both CNTK and LightGBM have been shown to have excellent training speed in a range of tasks (see, for examples, [this paper](https://arxiv.org/abs/1608.07249) for CNTK and [this summary](https://github.com/Microsoft/LightGBM/wiki/Experiments#comparison-experiment) for LightGBM), and this should help participants iterate on their models quickly.  Furthermore, we use [GPU acceleration available on Azure Data Science Virtual Machines](http://aka.ms/dsvm/deeplearning) to speed up computations in CNN featurization.  \n",
    "\n",
    "With these three tools, one can achieve a **score of 0.55979** on the leaderboard within approximately **two hours** (excluding data download and virtual machine startup time). \n",
    "\n",
    "\n",
    "This work is also published as a [blog in Technet](https://blogs.technet.microsoft.com/machinelearning/2017/02/17/quick-start-guide-to-the-data-science-bowl-lung-cancer-detection-challenge-using-deep-learning-microsoft-cognitive-toolkit-and-azure-gpu-vms/), as a [notebook in Cortana Gallery](https://gallery.cortanaintelligence.com/Notebook/Medical-Image-Recognition-for-the-Kaggle-Data-Science-Bowl-2017-with-CNTK-and-LightGBM-1) and as a [script in Kaggle](https://www.kaggle.com/hoaphumanoid/data-science-bowl-2017/cntk-and-lightgbm-quick-start/).  \n",
    "\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "The Azure Data Science Virtual Machine (DSVM) comes with CNTK, LightGBM, OpenCV, Scikit-learn preinstalled, so you don't have to worry about the configuration. \n",
    "\n",
    "Apart from the libraries, we are going to use a pretrained convolutional neural network (CNN) with the [ResNet architecture](https://arxiv.org/abs/1512.03385). This configuration, developed by Microsoft Research, was the first CNN to surpass the human level performance in image classification. The network was trained in the [ImageNet dataset](http://image-net.org/) and can be downloaded [here](https://migonzastorage.blob.core.windows.net/deep-learning/models/cntk/imagenet/ResNet_152.model).\n",
    "\n",
    "\n",
    "## Data\n",
    "In addition to the libraries you have to download the [data](https://www.kaggle.com/c/data-science-bowl-2017/data) of the competition. The images are in [DICOM](https://en.wikipedia.org/wiki/DICOM) format and consist of a group of horizontal slices of the thorax for each patient. This is a 3D reconstruction of a sample lung with the competition data. It was computed with this [script](https://www.kaggle.com/gzuidhof/data-science-bowl-2017/full-preprocessing-tutorial).Â  \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://migonzastorage.blob.core.windows.net/projects/data_science_bowl_2017/lung_bright.png\" alt=\"sample\" width=\"40%\"/>\n",
    "</p>\n",
    "\n",
    "The biggest file `stage1.7z` occupies 67Gb. When the file is uncompressed it occupies 141Gb. In case you need more space, you ca easily attach an external drive to your Azure VM. Here you can find the instructions for [Linux](https://docs.microsoft.com/en-us/azure/virtual-machines/virtual-machines-linux-attach-disk-portal?toc=%2fazure%2fvirtual-machines%2flinux%2ftoc.json). An alternative route is to attach a [fileshare disk](https://docs.microsoft.com/en-us/azure/storage/storage-how-to-use-files-linux). With [this script](https://github.com/miguelgfierro/scripts/blob/master/mount_azure_fileshare.sh) the fileshare can be attached in seconds in an Azure DSMV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.5.2 |Anaconda custom (64-bit)| (default, Jul  2 2016, 17:53:06) \n",
      "[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\n",
      "CNTK version: 2.0rc1\n"
     ]
    }
   ],
   "source": [
    "#Load libraries\n",
    "import sys,os\n",
    "import numpy as np\n",
    "import dicom\n",
    "import glob\n",
    "from sklearn import cross_validation\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import log_loss\n",
    "from matplotlib import pyplot as plt\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import time\n",
    "from cntk import load_model\n",
    "from cntk.ops import combine\n",
    "import pkg_resources\n",
    "from lightgbm.sklearn import LGBMRegressor\n",
    "from scipy.stats import gmean\n",
    "\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"CNTK version: {}\".format(pkg_resources.get_distribution(\"cntk\").version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Function definition\n",
    "\n",
    "The first step is to set the path and variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Put here the number of your experiment\n",
    "EXPERIMENT_NUMBER = '0042' \n",
    "\n",
    "#Put here the path to the downloaded ResNet model\n",
    "#https://migonzastorage.blob.core.windows.net/deep-learning/models/cntk/imagenet/ResNet_152.model\n",
    "MODEL_PATH='/datadrive/pretrained_models/ResNet_152.model' \n",
    "\n",
    "#Number of splits in K-fold cross validation\n",
    "N_SPLITS=5 \n",
    "\n",
    "#Put here the path where you downloaded all kaggle data\n",
    "DATA_PATH='/datadrive/lung_cancer'\n",
    "\n",
    "# Path and variables\n",
    "STAGE1_LABELS = os.path.join(DATA_PATH, 'stage1_labels.csv')\n",
    "STAGE1_SAMPLE_SUBMISSION = os.path.join(DATA_PATH, 'stage1_sample_submission.csv')\n",
    "STAGE1_FOLDER = os.path.join(DATA_PATH, 'stage1')\n",
    "EXPERIMENT_FOLDER = 'features%s' % EXPERIMENT_NUMBER\n",
    "FEATURE_FOLDER = os.path.join(DATA_PATH, 'features', EXPERIMENT_FOLDER)\n",
    "if not os.path.exists(FEATURE_FOLDER): os.makedirs(FEATURE_FOLDER)\n",
    "SUBMIT_OUTPUT='submit%s.csv' % EXPERIMENT_NUMBER\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is a timer class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Timer class\n",
    "class Timer(object):\n",
    "    def __enter__(self):\n",
    "        self.start()\n",
    "        return self\n",
    "    def __exit__(self, *args):\n",
    "        self.stop()\n",
    "    def start(self):\n",
    "        self.start = time.clock()\n",
    "    def stop(self):\n",
    "        self.end = time.clock()\n",
    "        self.interval = self.end - self.start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This couple of functions collect the images, perform an histogram equalization and are resized to the ImageNet standard size: `224x224`. Also, the images in ImageNet are color images, with three channels, RGB, however, the images from the data science competition are in gray scale. A quick way to adapt the cancer images to the format of ImageNet is to pack them in groups of three. That is what we are doing in the function `get_data_id`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_3d_data(path):\n",
    "    slices = [dicom.read_file(os.path.join(path, s)) for s in os.listdir(path)]\n",
    "    slices.sort(key=lambda x: int(x.InstanceNumber))\n",
    "    return np.stack([s.pixel_array for s in slices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_data_id(path, plot_data=False):\n",
    "    sample_image = get_3d_data(path)\n",
    "    sample_image[sample_image == -2000] = 0\n",
    "    if plot_data:\n",
    "        f, plots = plt.subplots(4, 5, sharex='col', sharey='row', figsize=(10, 8))\n",
    "\n",
    "    batch = []\n",
    "    cnt = 0\n",
    "    dx = 40\n",
    "    ds = 512\n",
    "    for i in range(0, sample_image.shape[0] - 3, 3):\n",
    "        tmp = []\n",
    "        for j in range(3):\n",
    "            img = sample_image[i + j]\n",
    "            img = 255.0 / np.amax(img) * img\n",
    "            img = cv2.equalizeHist(img.astype(np.uint8))\n",
    "            img = img[dx: ds - dx, dx: ds - dx]\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            tmp.append(img)\n",
    "\n",
    "        batch.append(tmp)\n",
    "\n",
    "        if plot_data:\n",
    "            if cnt < 20:\n",
    "                plots[cnt // 5, cnt % 5].axis('off')\n",
    "                tmp = np.array(tmp)\n",
    "                plots[cnt // 5, cnt % 5].imshow(tmp[0,:,:], cmap='gray')\n",
    "            cnt += 1\n",
    "\n",
    "    if plot_data: plt.show()\n",
    "        \n",
    "    batch = np.array(batch, dtype='int')\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "In the next figure, we show the complete architecture. Each patient has an arbitrary number of scan images. The images are cropped to `224x244` and packed in groups of 3, to match the format of ImageNet. They are fed to the pretrained network in k batches and then are convoluted in each internal layer, until the penultimate one. This process is preformed using CNTK. The output of the network are the features that we are going to feed to the boosted tree, programmed with LightGBM.Â  \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://migonzastorage.blob.core.windows.net:443/projects/data_science_bowl_2017/resnet_tree_lung2.png\" alt=\"sample\" width=\"60%\"/>\n",
    "</p>\n",
    "\n",
    "\n",
    "In this sample experiment, we used the last layer of a pretrained ResNet network with 152 layers as featurizer to extract features. CNTK provides other pretrained networks that you can test like [AlexNet](https://www.cntk.ai/Models/AlexNet/AlexNet.model), [AlexNet with Batch Normalization](https://www.cntk.ai/Models/AlexNet/AlexNetBS.model) and [ResNet with 18 layers](https://www.cntk.ai/Models/ResNet/ResNet_18.model).  \n",
    " \n",
    "Please note the name of the last layer of pretrained network is named as `z.x` in CNTK, which should be specified when use for featurizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_extractor():\n",
    "    node_name = \"z.x\"\n",
    "    loaded_model  = load_model(MODEL_PATH)\n",
    "    node_in_graph = loaded_model.find_by_name(node_name)\n",
    "    output_nodes  = combine([node_in_graph.owner])\n",
    "    return output_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This function is responsible of computing the features. For that, given a batch of images we just compute the forward propagation in the network. The resulting features are saved as a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def calc_features(verbose=False):\n",
    "    net = get_extractor()\n",
    "    for folder in glob.glob(os.path.join(STAGE1_FOLDER, '*')):\n",
    "        foldername = os.path.basename(folder)\n",
    "        if verbose: print(\"Computing features from patient {}\".format(foldername))\n",
    "        foldername_npy = foldername + '.npy'\n",
    "        if os.path.isfile(os.path.join(FEATURE_FOLDER, foldername_npy)):\n",
    "            if verbose: print(\"Features in %s already computed\" % (os.path.join(FEATURE_FOLDER, foldername)))\n",
    "            continue\n",
    "        batch = get_data_id(folder)\n",
    "        if verbose:\n",
    "            print(\"Batch size:\")\n",
    "            print(batch.shape)\n",
    "        feats = net.eval(batch)[0]\n",
    "        if verbose:\n",
    "            print(feats.shape)\n",
    "            print(\"Saving features in %s\" % os.path.join(FEATURE_FOLDER, foldername))\n",
    "        np.save(os.path.join(FEATURE_FOLDER, foldername), feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We need to perform a feature engineering process. We are going to perform a dimensionality reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Get the feature size of the network penultimante layer \n",
    "def get_feature_shape(df, verbose=True):\n",
    "    feat0 = np.load(os.path.join(FEATURE_FOLDER,'%s.npy' % str(df['id'].iloc[0]))).squeeze()\n",
    "    feat_shape = feat0.shape[1]\n",
    "    if verbose: print(\"Size of the penultimate layer (feature size): {}\".format(feat_shape))\n",
    "    return feat_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feature_engineering(filename, is_train, verbose=True):\n",
    "    df = pd.read_csv(filename)\n",
    "    if verbose: print(\"Training data of size {}\".format(df.shape))\n",
    "    feat_shape = get_feature_shape(df, verbose)\n",
    "    x = np.zeros((df.shape[0], feat_shape))\n",
    "    for i, id in enumerate(df['id'].tolist()):\n",
    "        feat = np.load(os.path.join(FEATURE_FOLDER,'%s.npy' % str(id))).squeeze()\n",
    "        feat_pca = PCA(n_components=1).fit_transform(feat.transpose())\n",
    "        x[i,:] = feat_pca.squeeze()\n",
    "    if is_train:\n",
    "        y = df['cancer'].as_matrix()\n",
    "    else:\n",
    "        y = None\n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can create a custom evaluation function for LightGBM. We want to use the same metric that is used by Kaggle to compute the score in the leaderboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Custom eval function expects a callable with following functions        \n",
    "def loglikelood(y_true, y_pred):\n",
    "    eval_result = log_loss(y_true, y_pred)\n",
    "    eval_name = 'log_loss'\n",
    "    is_bigger_better = False\n",
    "    return eval_name, eval_result, is_bigger_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have the features computed we can feed them in a boosted tree. Each numpy array contains the features of the images corresponding to one patient, and they are labelled as positive (the patient has cancer) or negative (the patient doesn't have cancer). We reduce the dimensionality of the features to obtain a one dimensional vector representing one observation. The metric used in the tree optimization is logloss, there are [other metrics](https://github.com/Microsoft/LightGBM/blob/7426ac3cbd9ae27b5d734a54e5e9880623beea43/docs/Parameters.md#metric-parameters) that can be applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def train_lightgbm(x, y, verbose=True):\n",
    "    skf = KFold(n_splits=N_SPLITS, random_state=2048, shuffle=True)\n",
    "    clfs = []\n",
    "    if verbose: print(\"Computing LightGBM boosted tree using {} kfold cross validation\".format(N_SPLITS))\n",
    "    for train_index, test_index in skf.split(x, y):\n",
    "        trn_x, val_x = x[train_index,:], x[test_index,:]\n",
    "        trn_y, val_y = y[train_index], y[test_index]\n",
    "\n",
    "        clf = LGBMRegressor(max_depth=6,\n",
    "                            num_leaves=21,\n",
    "                            n_estimators=5000,\n",
    "                            min_child_weight=30,\n",
    "                            learning_rate=0.01,\n",
    "                            nthread=24,\n",
    "                            boosting_type='gbdt',\n",
    "                            subsample=0.80,\n",
    "                            colsample_bytree=0.80,\n",
    "                            seed=42)\n",
    "\n",
    "        clf.fit(trn_x, trn_y, eval_set=[(val_x, val_y)], verbose=verbose, eval_metric=loglikelood, early_stopping_rounds=300)\n",
    "        clfs.append(clf)\n",
    "        \n",
    "    return clfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We create a wrapper of the training function. That way we make the code extendable in case another user wants to try other training method.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_training(verbose=True):\n",
    "    with Timer() as t:\n",
    "        x, y = feature_engineering(STAGE1_LABELS, is_train=True, verbose=verbose)\n",
    "        clf = train_lightgbm(x, y, verbose)\n",
    "    if verbose: print(\"Training took %.03f sec.\\n\" % t.interval)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have trained the model, we can use it to compute the results in the validation set. Again, this function is easily extendable in case a person want to use another model (always the scikit-learn interface `predict` has to be maintained)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def compute_prediction(clfs, verbose=True):\n",
    "    preds = []\n",
    "    with Timer() as t:\n",
    "        x, _ = feature_engineering(STAGE1_SAMPLE_SUBMISSION, is_train=False, verbose=verbose)\n",
    "        for clf in clfs:\n",
    "            preds.append(np.clip(clf.predict(x),0.0001,1))\n",
    "        pred = gmean(np.array(preds), axis=0)\n",
    "    if verbose: print(\"Prediction took %.03f sec.\\n\" % t.interval)        \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We finally save the results as a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def save_results(pred, verbose=True):\n",
    "    df = pd.read_csv(STAGE1_SAMPLE_SUBMISSION)\n",
    "    df['cancer'] = pred\n",
    "    df.to_csv(SUBMIT_OUTPUT, index=False)\n",
    "    if verbose: print(\"Results saved in {}\".format(SUBMIT_OUTPUT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Execution\n",
    "\n",
    "Plot an example of images with cancer. The plot shows horizontal slices of the thorax. This is the result.\n",
    "<p align=\"center\">\n",
    "<img src=\"https://migonzastorage.blob.core.windows.net/projects/data_science_bowl_2017/output_10_0.png\" alt=\"sample\" width=\"50%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch size:  (80, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "path_cancer = os.path.join(STAGE1_FOLDER, 'fe45462987bacc32dbc7126119999392')\n",
    "data_batch = get_data_id(path_cancer, False)\n",
    "print(\"Data batch size: \", data_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Plot example of images without cancer. This is the result:\n",
    "<p align=\"center\">\n",
    "<img src=\"https://migonzastorage.blob.core.windows.net/projects/data_science_bowl_2017/output_12_0.png\" alt=\"sample\" width=\"50%\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch size:  (57, 3, 224, 224)\n"
     ]
    }
   ],
   "source": [
    "path_no_cancer = os.path.join(STAGE1_FOLDER, 'ffe02fe7d2223743f7fb455dfaff3842')\n",
    "data_batch = get_data_id(path_no_cancer, False)\n",
    "print(\"Data batch size: \", data_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Get CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Composite(Combine): Input('features', [#, ], [3 x 224 x 224]) -> Output('z.x', [#, ], [2048 x 1 x 1])\n",
      "CPU times: user 6.1 s, sys: 3.76 s, total: 9.86 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feat = get_extractor()\n",
    "print(feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute features. Using a K80 GPU in an Azure NC24 the computation time was: `53min 7s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py35/lib/python3.5/site-packages/cntk/core.py:330: UserWarning: your data is of type \"int64\", but your input variable (uid \"Input4199\") expects \"<class 'numpy.float32'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 36min 44s, sys: 12min 29s, total: 49min 14s\n",
      "Wall time: 1h 52min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Calculate features\n",
    "calc_features(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Train the boosted tree. We train the tree with early stoping of 300 rounds, which means that the optimization will stop if the validation score doesn't improve for 300 rounds. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size (1397, 2)\n",
      "Size of the penultimate layer (feature size): 2048\n",
      "Computing LightGBM boosted tree using 5 kfold cross validation\n",
      "[1]\tvalid_0's log_loss: 0.562469\n",
      "Train until valid scores didn't improve in 300 rounds.\n",
      "[2]\tvalid_0's log_loss: 0.56237\n",
      "[3]\tvalid_0's log_loss: 0.562251\n",
      "[4]\tvalid_0's log_loss: 0.561818\n",
      "[5]\tvalid_0's log_loss: 0.561916\n",
      "[6]\tvalid_0's log_loss: 0.561888\n",
      "[7]\tvalid_0's log_loss: 0.561957\n",
      "[8]\tvalid_0's log_loss: 0.56183\n",
      "[9]\tvalid_0's log_loss: 0.561659\n",
      "[10]\tvalid_0's log_loss: 0.561511\n",
      "[11]\tvalid_0's log_loss: 0.561371\n",
      "[12]\tvalid_0's log_loss: 0.56134\n",
      "[13]\tvalid_0's log_loss: 0.561334\n",
      "[14]\tvalid_0's log_loss: 0.561059\n",
      "[15]\tvalid_0's log_loss: 0.561242\n",
      "[16]\tvalid_0's log_loss: 0.561401\n",
      "[17]\tvalid_0's log_loss: 0.56125\n",
      "[18]\tvalid_0's log_loss: 0.561094\n",
      "[19]\tvalid_0's log_loss: 0.560727\n",
      "[20]\tvalid_0's log_loss: 0.560579\n",
      "[21]\tvalid_0's log_loss: 0.560274\n",
      "[22]\tvalid_0's log_loss: 0.560587\n",
      "[23]\tvalid_0's log_loss: 0.560372\n",
      "[24]\tvalid_0's log_loss: 0.560257\n",
      "[25]\tvalid_0's log_loss: 0.560106\n",
      "[26]\tvalid_0's log_loss: 0.559466\n",
      "[27]\tvalid_0's log_loss: 0.5593\n",
      "[28]\tvalid_0's log_loss: 0.559054\n",
      "[29]\tvalid_0's log_loss: 0.559123\n",
      "[30]\tvalid_0's log_loss: 0.558996\n",
      "[31]\tvalid_0's log_loss: 0.558804\n",
      "[32]\tvalid_0's log_loss: 0.559069\n",
      "[33]\tvalid_0's log_loss: 0.55854\n",
      "[34]\tvalid_0's log_loss: 0.558438\n",
      "[35]\tvalid_0's log_loss: 0.55833\n",
      "[36]\tvalid_0's log_loss: 0.558335\n",
      "[37]\tvalid_0's log_loss: 0.55822\n",
      "[38]\tvalid_0's log_loss: 0.55816\n",
      "[39]\tvalid_0's log_loss: 0.558201\n",
      "[40]\tvalid_0's log_loss: 0.558437\n",
      "[41]\tvalid_0's log_loss: 0.558388\n",
      "[42]\tvalid_0's log_loss: 0.558351\n",
      "[43]\tvalid_0's log_loss: 0.558242\n",
      "[44]\tvalid_0's log_loss: 0.558203\n",
      "[45]\tvalid_0's log_loss: 0.558291\n",
      "[46]\tvalid_0's log_loss: 0.55813\n",
      "[47]\tvalid_0's log_loss: 0.557306\n",
      "[48]\tvalid_0's log_loss: 0.557505\n",
      "[49]\tvalid_0's log_loss: 0.557529\n",
      "[50]\tvalid_0's log_loss: 0.557219\n",
      "[51]\tvalid_0's log_loss: 0.55733\n",
      "[52]\tvalid_0's log_loss: 0.557443\n",
      "[53]\tvalid_0's log_loss: 0.557697\n",
      "[54]\tvalid_0's log_loss: 0.55767\n",
      "[55]\tvalid_0's log_loss: 0.557432\n",
      "[56]\tvalid_0's log_loss: 0.557489\n",
      "[57]\tvalid_0's log_loss: 0.557379\n",
      "[58]\tvalid_0's log_loss: 0.55712\n",
      "[59]\tvalid_0's log_loss: 0.557021\n",
      "[60]\tvalid_0's log_loss: 0.557152\n",
      "[61]\tvalid_0's log_loss: 0.557338\n",
      "[62]\tvalid_0's log_loss: 0.557763\n",
      "[63]\tvalid_0's log_loss: 0.558218\n",
      "[64]\tvalid_0's log_loss: 0.558463\n",
      "[65]\tvalid_0's log_loss: 0.558602\n",
      "[66]\tvalid_0's log_loss: 0.558732\n",
      "[67]\tvalid_0's log_loss: 0.558578\n",
      "[68]\tvalid_0's log_loss: 0.55849\n",
      "[69]\tvalid_0's log_loss: 0.558561\n",
      "[70]\tvalid_0's log_loss: 0.558177\n",
      "[71]\tvalid_0's log_loss: 0.558678\n",
      "[72]\tvalid_0's log_loss: 0.558756\n",
      "[73]\tvalid_0's log_loss: 0.558885\n",
      "[74]\tvalid_0's log_loss: 0.559429\n",
      "[75]\tvalid_0's log_loss: 0.559381\n",
      "[76]\tvalid_0's log_loss: 0.559193\n",
      "[77]\tvalid_0's log_loss: 0.558916\n",
      "[78]\tvalid_0's log_loss: 0.559182\n",
      "[79]\tvalid_0's log_loss: 0.559031\n",
      "[80]\tvalid_0's log_loss: 0.559012\n",
      "[81]\tvalid_0's log_loss: 0.558906\n",
      "[82]\tvalid_0's log_loss: 0.558847\n",
      "[83]\tvalid_0's log_loss: 0.559004\n",
      "[84]\tvalid_0's log_loss: 0.559154\n",
      "[85]\tvalid_0's log_loss: 0.559262\n",
      "[86]\tvalid_0's log_loss: 0.559182\n",
      "[87]\tvalid_0's log_loss: 0.55915\n",
      "[88]\tvalid_0's log_loss: 0.559612\n",
      "[89]\tvalid_0's log_loss: 0.559697\n",
      "[90]\tvalid_0's log_loss: 0.55972\n",
      "[91]\tvalid_0's log_loss: 0.559624\n",
      "[92]\tvalid_0's log_loss: 0.559534\n",
      "[93]\tvalid_0's log_loss: 0.5595\n",
      "[94]\tvalid_0's log_loss: 0.559699\n",
      "[95]\tvalid_0's log_loss: 0.559597\n",
      "[96]\tvalid_0's log_loss: 0.559559\n",
      "[97]\tvalid_0's log_loss: 0.559905\n",
      "[98]\tvalid_0's log_loss: 0.559876\n",
      "[99]\tvalid_0's log_loss: 0.56038\n",
      "[100]\tvalid_0's log_loss: 0.560392\n",
      "[101]\tvalid_0's log_loss: 0.560512\n",
      "[102]\tvalid_0's log_loss: 0.5606\n",
      "[103]\tvalid_0's log_loss: 0.560729\n",
      "[104]\tvalid_0's log_loss: 0.561015\n",
      "[105]\tvalid_0's log_loss: 0.560963\n",
      "[106]\tvalid_0's log_loss: 0.560942\n",
      "[107]\tvalid_0's log_loss: 0.561266\n",
      "[108]\tvalid_0's log_loss: 0.561322\n",
      "[109]\tvalid_0's log_loss: 0.561407\n",
      "[110]\tvalid_0's log_loss: 0.561094\n",
      "[111]\tvalid_0's log_loss: 0.560951\n",
      "[112]\tvalid_0's log_loss: 0.561112\n",
      "[113]\tvalid_0's log_loss: 0.561269\n",
      "[114]\tvalid_0's log_loss: 0.561539\n",
      "[115]\tvalid_0's log_loss: 0.561392\n",
      "[116]\tvalid_0's log_loss: 0.561494\n",
      "[117]\tvalid_0's log_loss: 0.561514\n",
      "[118]\tvalid_0's log_loss: 0.561628\n",
      "[119]\tvalid_0's log_loss: 0.56181\n",
      "[120]\tvalid_0's log_loss: 0.561337\n",
      "[121]\tvalid_0's log_loss: 0.561238\n",
      "[122]\tvalid_0's log_loss: 0.56116\n",
      "[123]\tvalid_0's log_loss: 0.561187\n",
      "[124]\tvalid_0's log_loss: 0.561306\n",
      "[125]\tvalid_0's log_loss: 0.561105\n",
      "[126]\tvalid_0's log_loss: 0.561285\n",
      "[127]\tvalid_0's log_loss: 0.56143\n",
      "[128]\tvalid_0's log_loss: 0.561623\n",
      "[129]\tvalid_0's log_loss: 0.561626\n",
      "[130]\tvalid_0's log_loss: 0.561297\n",
      "[131]\tvalid_0's log_loss: 0.561188\n",
      "[132]\tvalid_0's log_loss: 0.56154\n",
      "[133]\tvalid_0's log_loss: 0.561576\n",
      "[134]\tvalid_0's log_loss: 0.56177\n",
      "[135]\tvalid_0's log_loss: 0.561568\n",
      "[136]\tvalid_0's log_loss: 0.561219\n",
      "[137]\tvalid_0's log_loss: 0.56133\n",
      "[138]\tvalid_0's log_loss: 0.561182\n",
      "[139]\tvalid_0's log_loss: 0.561344\n",
      "[140]\tvalid_0's log_loss: 0.561206\n",
      "[141]\tvalid_0's log_loss: 0.561394\n",
      "[142]\tvalid_0's log_loss: 0.561418\n",
      "[143]\tvalid_0's log_loss: 0.561251\n",
      "[144]\tvalid_0's log_loss: 0.561447\n",
      "[145]\tvalid_0's log_loss: 0.562027\n",
      "[146]\tvalid_0's log_loss: 0.562135\n",
      "[147]\tvalid_0's log_loss: 0.561648\n",
      "[148]\tvalid_0's log_loss: 0.56152\n",
      "[149]\tvalid_0's log_loss: 0.561692\n",
      "[150]\tvalid_0's log_loss: 0.561783\n",
      "[151]\tvalid_0's log_loss: 0.561897\n",
      "[152]\tvalid_0's log_loss: 0.562022\n",
      "[153]\tvalid_0's log_loss: 0.562212\n",
      "[154]\tvalid_0's log_loss: 0.562221\n",
      "[155]\tvalid_0's log_loss: 0.562182\n",
      "[156]\tvalid_0's log_loss: 0.562073\n",
      "[157]\tvalid_0's log_loss: 0.562111\n",
      "[158]\tvalid_0's log_loss: 0.562099\n",
      "[159]\tvalid_0's log_loss: 0.561973\n",
      "[160]\tvalid_0's log_loss: 0.561937\n",
      "[161]\tvalid_0's log_loss: 0.561881\n",
      "[162]\tvalid_0's log_loss: 0.562237\n",
      "[163]\tvalid_0's log_loss: 0.562462\n",
      "[164]\tvalid_0's log_loss: 0.562723\n",
      "[165]\tvalid_0's log_loss: 0.562741\n",
      "[166]\tvalid_0's log_loss: 0.562603\n",
      "[167]\tvalid_0's log_loss: 0.563061\n",
      "[168]\tvalid_0's log_loss: 0.563071\n",
      "[169]\tvalid_0's log_loss: 0.563017\n",
      "[170]\tvalid_0's log_loss: 0.562931\n",
      "[171]\tvalid_0's log_loss: 0.562838\n",
      "[172]\tvalid_0's log_loss: 0.563071\n",
      "[173]\tvalid_0's log_loss: 0.563553\n",
      "[174]\tvalid_0's log_loss: 0.563594\n",
      "[175]\tvalid_0's log_loss: 0.56385\n",
      "[176]\tvalid_0's log_loss: 0.563999\n",
      "[177]\tvalid_0's log_loss: 0.563782\n",
      "[178]\tvalid_0's log_loss: 0.563865\n",
      "[179]\tvalid_0's log_loss: 0.564102\n",
      "[180]\tvalid_0's log_loss: 0.563788\n",
      "[181]\tvalid_0's log_loss: 0.563944\n",
      "[182]\tvalid_0's log_loss: 0.563832\n",
      "[183]\tvalid_0's log_loss: 0.564257\n",
      "[184]\tvalid_0's log_loss: 0.564382\n",
      "[185]\tvalid_0's log_loss: 0.564665\n",
      "[186]\tvalid_0's log_loss: 0.564729\n",
      "[187]\tvalid_0's log_loss: 0.564965\n",
      "[188]\tvalid_0's log_loss: 0.565119\n",
      "[189]\tvalid_0's log_loss: 0.56509\n",
      "[190]\tvalid_0's log_loss: 0.565266\n",
      "[191]\tvalid_0's log_loss: 0.565241\n",
      "[192]\tvalid_0's log_loss: 0.565311\n",
      "[193]\tvalid_0's log_loss: 0.565217\n",
      "[194]\tvalid_0's log_loss: 0.565412\n",
      "[195]\tvalid_0's log_loss: 0.565055\n",
      "[196]\tvalid_0's log_loss: 0.565131\n",
      "[197]\tvalid_0's log_loss: 0.5651\n",
      "[198]\tvalid_0's log_loss: 0.565425\n",
      "[199]\tvalid_0's log_loss: 0.565434\n",
      "[200]\tvalid_0's log_loss: 0.565727\n",
      "[201]\tvalid_0's log_loss: 0.565597\n",
      "[202]\tvalid_0's log_loss: 0.565829\n",
      "[203]\tvalid_0's log_loss: 0.565854\n",
      "[204]\tvalid_0's log_loss: 0.56598\n",
      "[205]\tvalid_0's log_loss: 0.566004\n",
      "[206]\tvalid_0's log_loss: 0.566093\n",
      "[207]\tvalid_0's log_loss: 0.566093\n",
      "[208]\tvalid_0's log_loss: 0.5662\n",
      "[209]\tvalid_0's log_loss: 0.566084\n",
      "[210]\tvalid_0's log_loss: 0.566266\n",
      "[211]\tvalid_0's log_loss: 0.565873\n",
      "[212]\tvalid_0's log_loss: 0.566405\n",
      "[213]\tvalid_0's log_loss: 0.566618\n",
      "[214]\tvalid_0's log_loss: 0.56673\n",
      "[215]\tvalid_0's log_loss: 0.566826\n",
      "[216]\tvalid_0's log_loss: 0.566928\n",
      "[217]\tvalid_0's log_loss: 0.567128\n",
      "[218]\tvalid_0's log_loss: 0.567205\n",
      "[219]\tvalid_0's log_loss: 0.567265\n",
      "[220]\tvalid_0's log_loss: 0.567502\n",
      "[221]\tvalid_0's log_loss: 0.567382\n",
      "[222]\tvalid_0's log_loss: 0.567533\n",
      "[223]\tvalid_0's log_loss: 0.56752\n",
      "[224]\tvalid_0's log_loss: 0.567435\n",
      "[225]\tvalid_0's log_loss: 0.567431\n",
      "[226]\tvalid_0's log_loss: 0.567549\n",
      "[227]\tvalid_0's log_loss: 0.567595\n",
      "[228]\tvalid_0's log_loss: 0.567716\n",
      "[229]\tvalid_0's log_loss: 0.567638\n",
      "[230]\tvalid_0's log_loss: 0.5678\n",
      "[231]\tvalid_0's log_loss: 0.567615\n",
      "[232]\tvalid_0's log_loss: 0.567687\n",
      "[233]\tvalid_0's log_loss: 0.567633\n",
      "[234]\tvalid_0's log_loss: 0.567448\n",
      "[235]\tvalid_0's log_loss: 0.56738\n",
      "[236]\tvalid_0's log_loss: 0.567292\n",
      "[237]\tvalid_0's log_loss: 0.566603\n",
      "[238]\tvalid_0's log_loss: 0.566771\n",
      "[239]\tvalid_0's log_loss: 0.566845\n",
      "[240]\tvalid_0's log_loss: 0.566903\n",
      "[241]\tvalid_0's log_loss: 0.567102\n",
      "[242]\tvalid_0's log_loss: 0.567086\n",
      "[243]\tvalid_0's log_loss: 0.567141\n",
      "[244]\tvalid_0's log_loss: 0.567286\n",
      "[245]\tvalid_0's log_loss: 0.567526\n",
      "[246]\tvalid_0's log_loss: 0.567093\n",
      "[247]\tvalid_0's log_loss: 0.567046\n",
      "[248]\tvalid_0's log_loss: 0.567019\n",
      "[249]\tvalid_0's log_loss: 0.567259\n",
      "[250]\tvalid_0's log_loss: 0.567243\n",
      "[251]\tvalid_0's log_loss: 0.567441\n",
      "[252]\tvalid_0's log_loss: 0.567326\n",
      "[253]\tvalid_0's log_loss: 0.567783\n",
      "[254]\tvalid_0's log_loss: 0.567886\n",
      "[255]\tvalid_0's log_loss: 0.568128\n",
      "[256]\tvalid_0's log_loss: 0.568027\n",
      "[257]\tvalid_0's log_loss: 0.568444\n",
      "[258]\tvalid_0's log_loss: 0.568921\n",
      "[259]\tvalid_0's log_loss: 0.569439\n",
      "[260]\tvalid_0's log_loss: 0.569797\n",
      "[261]\tvalid_0's log_loss: 0.569639\n",
      "[262]\tvalid_0's log_loss: 0.569577\n",
      "[263]\tvalid_0's log_loss: 0.569838\n",
      "[264]\tvalid_0's log_loss: 0.570437\n",
      "[265]\tvalid_0's log_loss: 0.570974\n",
      "[266]\tvalid_0's log_loss: 0.571002\n",
      "[267]\tvalid_0's log_loss: 0.570774\n",
      "[268]\tvalid_0's log_loss: 0.571008\n",
      "[269]\tvalid_0's log_loss: 0.570987\n",
      "[270]\tvalid_0's log_loss: 0.571175\n",
      "[271]\tvalid_0's log_loss: 0.571513\n",
      "[272]\tvalid_0's log_loss: 0.571532\n",
      "[273]\tvalid_0's log_loss: 0.57116\n",
      "[274]\tvalid_0's log_loss: 0.571665\n",
      "[275]\tvalid_0's log_loss: 0.571567\n",
      "[276]\tvalid_0's log_loss: 0.571607\n",
      "[277]\tvalid_0's log_loss: 0.571942\n",
      "[278]\tvalid_0's log_loss: 0.571877\n",
      "[279]\tvalid_0's log_loss: 0.571934\n",
      "[280]\tvalid_0's log_loss: 0.57194\n",
      "[281]\tvalid_0's log_loss: 0.572073\n",
      "[282]\tvalid_0's log_loss: 0.572257\n",
      "[283]\tvalid_0's log_loss: 0.57239\n",
      "[284]\tvalid_0's log_loss: 0.572603\n",
      "[285]\tvalid_0's log_loss: 0.57259\n",
      "[286]\tvalid_0's log_loss: 0.572738\n",
      "[287]\tvalid_0's log_loss: 0.573139\n",
      "[288]\tvalid_0's log_loss: 0.573006\n",
      "[289]\tvalid_0's log_loss: 0.573475\n",
      "[290]\tvalid_0's log_loss: 0.573518\n",
      "[291]\tvalid_0's log_loss: 0.573632\n",
      "[292]\tvalid_0's log_loss: 0.573732\n",
      "[293]\tvalid_0's log_loss: 0.573933\n",
      "[294]\tvalid_0's log_loss: 0.573671\n",
      "[295]\tvalid_0's log_loss: 0.573663\n",
      "[296]\tvalid_0's log_loss: 0.573716\n",
      "[297]\tvalid_0's log_loss: 0.573618\n",
      "[298]\tvalid_0's log_loss: 0.573605\n",
      "[299]\tvalid_0's log_loss: 0.573519\n",
      "[300]\tvalid_0's log_loss: 0.573733\n",
      "[301]\tvalid_0's log_loss: 0.573557\n",
      "[302]\tvalid_0's log_loss: 0.573115\n",
      "[303]\tvalid_0's log_loss: 0.573413\n",
      "[304]\tvalid_0's log_loss: 0.573489\n",
      "[305]\tvalid_0's log_loss: 0.573837\n",
      "[306]\tvalid_0's log_loss: 0.574342\n",
      "[307]\tvalid_0's log_loss: 0.574456\n",
      "[308]\tvalid_0's log_loss: 0.574354\n",
      "[309]\tvalid_0's log_loss: 0.574225\n",
      "[310]\tvalid_0's log_loss: 0.57448\n",
      "[311]\tvalid_0's log_loss: 0.57465\n",
      "[312]\tvalid_0's log_loss: 0.57485\n",
      "[313]\tvalid_0's log_loss: 0.574932\n",
      "[314]\tvalid_0's log_loss: 0.57434\n",
      "[315]\tvalid_0's log_loss: 0.574257\n",
      "[316]\tvalid_0's log_loss: 0.574541\n",
      "[317]\tvalid_0's log_loss: 0.574242\n",
      "[318]\tvalid_0's log_loss: 0.574208\n",
      "[319]\tvalid_0's log_loss: 0.574587\n",
      "[320]\tvalid_0's log_loss: 0.575228\n",
      "[321]\tvalid_0's log_loss: 0.575369\n",
      "[322]\tvalid_0's log_loss: 0.575672\n",
      "[323]\tvalid_0's log_loss: 0.57628\n",
      "[324]\tvalid_0's log_loss: 0.576565\n",
      "[325]\tvalid_0's log_loss: 0.57689\n",
      "[326]\tvalid_0's log_loss: 0.576593\n",
      "[327]\tvalid_0's log_loss: 0.576673\n",
      "[328]\tvalid_0's log_loss: 0.57653\n",
      "[329]\tvalid_0's log_loss: 0.576154\n",
      "[330]\tvalid_0's log_loss: 0.576637\n",
      "[331]\tvalid_0's log_loss: 0.57638\n",
      "[332]\tvalid_0's log_loss: 0.576748\n",
      "[333]\tvalid_0's log_loss: 0.577042\n",
      "[334]\tvalid_0's log_loss: 0.577628\n",
      "[335]\tvalid_0's log_loss: 0.578092\n",
      "[336]\tvalid_0's log_loss: 0.577884\n",
      "[337]\tvalid_0's log_loss: 0.578126\n",
      "[338]\tvalid_0's log_loss: 0.579017\n",
      "[339]\tvalid_0's log_loss: 0.579455\n",
      "[340]\tvalid_0's log_loss: 0.5804\n",
      "[341]\tvalid_0's log_loss: 0.580503\n",
      "[342]\tvalid_0's log_loss: 0.579989\n",
      "[343]\tvalid_0's log_loss: 0.578849\n",
      "[344]\tvalid_0's log_loss: 0.579616\n",
      "[345]\tvalid_0's log_loss: 0.578603\n",
      "[346]\tvalid_0's log_loss: 0.578584\n",
      "[347]\tvalid_0's log_loss: 0.578785\n",
      "[348]\tvalid_0's log_loss: 0.578919\n",
      "[349]\tvalid_0's log_loss: 0.57849\n",
      "[350]\tvalid_0's log_loss: 0.578579\n",
      "[351]\tvalid_0's log_loss: 0.57954\n",
      "[352]\tvalid_0's log_loss: 0.57916\n",
      "[353]\tvalid_0's log_loss: 0.580156\n",
      "[354]\tvalid_0's log_loss: 0.580364\n",
      "[355]\tvalid_0's log_loss: 0.581748\n",
      "[356]\tvalid_0's log_loss: 0.58051\n",
      "[357]\tvalid_0's log_loss: 0.580772\n",
      "[358]\tvalid_0's log_loss: 0.580362\n",
      "[359]\tvalid_0's log_loss: 0.580827\n",
      "Early stopping, best iteration is:\n",
      "[59]\tvalid_0's log_loss: 0.557021\n",
      "[1]\tvalid_0's log_loss: 0.588825\n",
      "Train until valid scores didn't improve in 300 rounds.\n",
      "[2]\tvalid_0's log_loss: 0.588575\n",
      "[3]\tvalid_0's log_loss: 0.588674\n",
      "[4]\tvalid_0's log_loss: 0.588654\n",
      "[5]\tvalid_0's log_loss: 0.588522\n",
      "[6]\tvalid_0's log_loss: 0.587987\n",
      "[7]\tvalid_0's log_loss: 0.587395\n",
      "[8]\tvalid_0's log_loss: 0.587173\n",
      "[9]\tvalid_0's log_loss: 0.586871\n",
      "[10]\tvalid_0's log_loss: 0.587043\n",
      "[11]\tvalid_0's log_loss: 0.587045\n",
      "[12]\tvalid_0's log_loss: 0.587439\n",
      "[13]\tvalid_0's log_loss: 0.58725\n",
      "[14]\tvalid_0's log_loss: 0.587366\n",
      "[15]\tvalid_0's log_loss: 0.587412\n",
      "[16]\tvalid_0's log_loss: 0.587179\n",
      "[17]\tvalid_0's log_loss: 0.587439\n",
      "[18]\tvalid_0's log_loss: 0.587359\n",
      "[19]\tvalid_0's log_loss: 0.587206\n",
      "[20]\tvalid_0's log_loss: 0.586602\n",
      "[21]\tvalid_0's log_loss: 0.586457\n",
      "[22]\tvalid_0's log_loss: 0.586236\n",
      "[23]\tvalid_0's log_loss: 0.586668\n",
      "[24]\tvalid_0's log_loss: 0.586024\n",
      "[25]\tvalid_0's log_loss: 0.586414\n",
      "[26]\tvalid_0's log_loss: 0.586243\n",
      "[27]\tvalid_0's log_loss: 0.586324\n",
      "[28]\tvalid_0's log_loss: 0.586093\n",
      "[29]\tvalid_0's log_loss: 0.586069\n",
      "[30]\tvalid_0's log_loss: 0.586169\n",
      "[31]\tvalid_0's log_loss: 0.586607\n",
      "[32]\tvalid_0's log_loss: 0.586481\n",
      "[33]\tvalid_0's log_loss: 0.586296\n",
      "[34]\tvalid_0's log_loss: 0.586289\n",
      "[35]\tvalid_0's log_loss: 0.586502\n",
      "[36]\tvalid_0's log_loss: 0.586475\n",
      "[37]\tvalid_0's log_loss: 0.586682\n",
      "[38]\tvalid_0's log_loss: 0.586202\n",
      "[39]\tvalid_0's log_loss: 0.585757\n",
      "[40]\tvalid_0's log_loss: 0.585388\n",
      "[41]\tvalid_0's log_loss: 0.585329\n",
      "[42]\tvalid_0's log_loss: 0.585504\n",
      "[43]\tvalid_0's log_loss: 0.585106\n",
      "[44]\tvalid_0's log_loss: 0.584935\n",
      "[45]\tvalid_0's log_loss: 0.584646\n",
      "[46]\tvalid_0's log_loss: 0.584613\n",
      "[47]\tvalid_0's log_loss: 0.584231\n",
      "[48]\tvalid_0's log_loss: 0.584279\n",
      "[49]\tvalid_0's log_loss: 0.584213\n",
      "[50]\tvalid_0's log_loss: 0.58413\n",
      "[51]\tvalid_0's log_loss: 0.583991\n",
      "[52]\tvalid_0's log_loss: 0.584048\n",
      "[53]\tvalid_0's log_loss: 0.584114\n",
      "[54]\tvalid_0's log_loss: 0.584241\n",
      "[55]\tvalid_0's log_loss: 0.58415\n",
      "[56]\tvalid_0's log_loss: 0.583804\n",
      "[57]\tvalid_0's log_loss: 0.583833\n",
      "[58]\tvalid_0's log_loss: 0.58365\n",
      "[59]\tvalid_0's log_loss: 0.583504\n",
      "[60]\tvalid_0's log_loss: 0.583414\n",
      "[61]\tvalid_0's log_loss: 0.583139\n",
      "[62]\tvalid_0's log_loss: 0.583428\n",
      "[63]\tvalid_0's log_loss: 0.583599\n",
      "[64]\tvalid_0's log_loss: 0.584081\n",
      "[65]\tvalid_0's log_loss: 0.584106\n",
      "[66]\tvalid_0's log_loss: 0.583759\n",
      "[67]\tvalid_0's log_loss: 0.583615\n",
      "[68]\tvalid_0's log_loss: 0.583691\n",
      "[69]\tvalid_0's log_loss: 0.583704\n",
      "[70]\tvalid_0's log_loss: 0.583844\n",
      "[71]\tvalid_0's log_loss: 0.583397\n",
      "[72]\tvalid_0's log_loss: 0.583308\n",
      "[73]\tvalid_0's log_loss: 0.583451\n",
      "[74]\tvalid_0's log_loss: 0.583092\n",
      "[75]\tvalid_0's log_loss: 0.583255\n",
      "[76]\tvalid_0's log_loss: 0.58308\n",
      "[77]\tvalid_0's log_loss: 0.583085\n",
      "[78]\tvalid_0's log_loss: 0.582957\n",
      "[79]\tvalid_0's log_loss: 0.583032\n",
      "[80]\tvalid_0's log_loss: 0.583015\n",
      "[81]\tvalid_0's log_loss: 0.583429\n",
      "[82]\tvalid_0's log_loss: 0.583153\n",
      "[83]\tvalid_0's log_loss: 0.583477\n",
      "[84]\tvalid_0's log_loss: 0.583153\n",
      "[85]\tvalid_0's log_loss: 0.582935\n",
      "[86]\tvalid_0's log_loss: 0.583047\n",
      "[87]\tvalid_0's log_loss: 0.583052\n",
      "[88]\tvalid_0's log_loss: 0.582784\n",
      "[89]\tvalid_0's log_loss: 0.582586\n",
      "[90]\tvalid_0's log_loss: 0.58298\n",
      "[91]\tvalid_0's log_loss: 0.583125\n",
      "[92]\tvalid_0's log_loss: 0.583016\n",
      "[93]\tvalid_0's log_loss: 0.583279\n",
      "[94]\tvalid_0's log_loss: 0.582927\n",
      "[95]\tvalid_0's log_loss: 0.583172\n",
      "[96]\tvalid_0's log_loss: 0.583243\n",
      "[97]\tvalid_0's log_loss: 0.583494\n",
      "[98]\tvalid_0's log_loss: 0.58355\n",
      "[99]\tvalid_0's log_loss: 0.583307\n",
      "[100]\tvalid_0's log_loss: 0.583069\n",
      "[101]\tvalid_0's log_loss: 0.583208\n",
      "[102]\tvalid_0's log_loss: 0.583048\n",
      "[103]\tvalid_0's log_loss: 0.583187\n",
      "[104]\tvalid_0's log_loss: 0.583303\n",
      "[105]\tvalid_0's log_loss: 0.58309\n",
      "[106]\tvalid_0's log_loss: 0.583146\n",
      "[107]\tvalid_0's log_loss: 0.583062\n",
      "[108]\tvalid_0's log_loss: 0.582694\n",
      "[109]\tvalid_0's log_loss: 0.582415\n",
      "[110]\tvalid_0's log_loss: 0.582596\n",
      "[111]\tvalid_0's log_loss: 0.582777\n",
      "[112]\tvalid_0's log_loss: 0.582882\n",
      "[113]\tvalid_0's log_loss: 0.58296\n",
      "[114]\tvalid_0's log_loss: 0.582756\n",
      "[115]\tvalid_0's log_loss: 0.582452\n",
      "[116]\tvalid_0's log_loss: 0.582582\n",
      "[117]\tvalid_0's log_loss: 0.582593\n",
      "[118]\tvalid_0's log_loss: 0.58188\n",
      "[119]\tvalid_0's log_loss: 0.581909\n",
      "[120]\tvalid_0's log_loss: 0.581668\n",
      "[121]\tvalid_0's log_loss: 0.581816\n",
      "[122]\tvalid_0's log_loss: 0.582066\n",
      "[123]\tvalid_0's log_loss: 0.582166\n",
      "[124]\tvalid_0's log_loss: 0.581906\n",
      "[125]\tvalid_0's log_loss: 0.581757\n",
      "[126]\tvalid_0's log_loss: 0.581996\n",
      "[127]\tvalid_0's log_loss: 0.58206\n",
      "[128]\tvalid_0's log_loss: 0.582111\n",
      "[129]\tvalid_0's log_loss: 0.582066\n",
      "[130]\tvalid_0's log_loss: 0.58225\n",
      "[131]\tvalid_0's log_loss: 0.58256\n",
      "[132]\tvalid_0's log_loss: 0.582475\n",
      "[133]\tvalid_0's log_loss: 0.582361\n",
      "[134]\tvalid_0's log_loss: 0.582709\n",
      "[135]\tvalid_0's log_loss: 0.582696\n",
      "[136]\tvalid_0's log_loss: 0.582944\n",
      "[137]\tvalid_0's log_loss: 0.582843\n",
      "[138]\tvalid_0's log_loss: 0.582704\n",
      "[139]\tvalid_0's log_loss: 0.582281\n",
      "[140]\tvalid_0's log_loss: 0.582712\n",
      "[141]\tvalid_0's log_loss: 0.582883\n",
      "[142]\tvalid_0's log_loss: 0.582825\n",
      "[143]\tvalid_0's log_loss: 0.582687\n",
      "[144]\tvalid_0's log_loss: 0.582754\n",
      "[145]\tvalid_0's log_loss: 0.58289\n",
      "[146]\tvalid_0's log_loss: 0.582751\n",
      "[147]\tvalid_0's log_loss: 0.582954\n",
      "[148]\tvalid_0's log_loss: 0.582999\n",
      "[149]\tvalid_0's log_loss: 0.583333\n",
      "[150]\tvalid_0's log_loss: 0.58333\n",
      "[151]\tvalid_0's log_loss: 0.583502\n",
      "[152]\tvalid_0's log_loss: 0.583388\n",
      "[153]\tvalid_0's log_loss: 0.583367\n",
      "[154]\tvalid_0's log_loss: 0.583116\n",
      "[155]\tvalid_0's log_loss: 0.583029\n",
      "[156]\tvalid_0's log_loss: 0.583205\n",
      "[157]\tvalid_0's log_loss: 0.582906\n",
      "[158]\tvalid_0's log_loss: 0.582478\n",
      "[159]\tvalid_0's log_loss: 0.582496\n",
      "[160]\tvalid_0's log_loss: 0.582688\n",
      "[161]\tvalid_0's log_loss: 0.582689\n",
      "[162]\tvalid_0's log_loss: 0.582451\n",
      "[163]\tvalid_0's log_loss: 0.582634\n",
      "[164]\tvalid_0's log_loss: 0.582657\n",
      "[165]\tvalid_0's log_loss: 0.582949\n",
      "[166]\tvalid_0's log_loss: 0.582906\n",
      "[167]\tvalid_0's log_loss: 0.582921\n",
      "[168]\tvalid_0's log_loss: 0.582889\n",
      "[169]\tvalid_0's log_loss: 0.58308\n",
      "[170]\tvalid_0's log_loss: 0.582984\n",
      "[171]\tvalid_0's log_loss: 0.582863\n",
      "[172]\tvalid_0's log_loss: 0.582883\n",
      "[173]\tvalid_0's log_loss: 0.582914\n",
      "[174]\tvalid_0's log_loss: 0.582664\n",
      "[175]\tvalid_0's log_loss: 0.582701\n",
      "[176]\tvalid_0's log_loss: 0.582766\n",
      "[177]\tvalid_0's log_loss: 0.58294\n",
      "[178]\tvalid_0's log_loss: 0.58301\n",
      "[179]\tvalid_0's log_loss: 0.58306\n",
      "[180]\tvalid_0's log_loss: 0.58325\n",
      "[181]\tvalid_0's log_loss: 0.583059\n",
      "[182]\tvalid_0's log_loss: 0.583169\n",
      "[183]\tvalid_0's log_loss: 0.582867\n",
      "[184]\tvalid_0's log_loss: 0.583354\n",
      "[185]\tvalid_0's log_loss: 0.583714\n",
      "[186]\tvalid_0's log_loss: 0.583628\n",
      "[187]\tvalid_0's log_loss: 0.583714\n",
      "[188]\tvalid_0's log_loss: 0.583676\n",
      "[189]\tvalid_0's log_loss: 0.583376\n",
      "[190]\tvalid_0's log_loss: 0.583413\n",
      "[191]\tvalid_0's log_loss: 0.583466\n",
      "[192]\tvalid_0's log_loss: 0.583693\n",
      "[193]\tvalid_0's log_loss: 0.583714\n",
      "[194]\tvalid_0's log_loss: 0.583958\n",
      "[195]\tvalid_0's log_loss: 0.584199\n",
      "[196]\tvalid_0's log_loss: 0.584379\n",
      "[197]\tvalid_0's log_loss: 0.584617\n",
      "[198]\tvalid_0's log_loss: 0.584718\n",
      "[199]\tvalid_0's log_loss: 0.584902\n",
      "[200]\tvalid_0's log_loss: 0.584877\n",
      "[201]\tvalid_0's log_loss: 0.585082\n",
      "[202]\tvalid_0's log_loss: 0.58515\n",
      "[203]\tvalid_0's log_loss: 0.585074\n",
      "[204]\tvalid_0's log_loss: 0.585014\n",
      "[205]\tvalid_0's log_loss: 0.584895\n",
      "[206]\tvalid_0's log_loss: 0.584859\n",
      "[207]\tvalid_0's log_loss: 0.584836\n",
      "[208]\tvalid_0's log_loss: 0.584919\n",
      "[209]\tvalid_0's log_loss: 0.585015\n",
      "[210]\tvalid_0's log_loss: 0.585064\n",
      "[211]\tvalid_0's log_loss: 0.585408\n",
      "[212]\tvalid_0's log_loss: 0.585105\n",
      "[213]\tvalid_0's log_loss: 0.585266\n",
      "[214]\tvalid_0's log_loss: 0.585378\n",
      "[215]\tvalid_0's log_loss: 0.585268\n",
      "[216]\tvalid_0's log_loss: 0.585555\n",
      "[217]\tvalid_0's log_loss: 0.585547\n",
      "[218]\tvalid_0's log_loss: 0.585739\n",
      "[219]\tvalid_0's log_loss: 0.585938\n",
      "[220]\tvalid_0's log_loss: 0.586167\n",
      "[221]\tvalid_0's log_loss: 0.585976\n",
      "[222]\tvalid_0's log_loss: 0.585924\n",
      "[223]\tvalid_0's log_loss: 0.58576\n",
      "[224]\tvalid_0's log_loss: 0.585962\n",
      "[225]\tvalid_0's log_loss: 0.585898\n",
      "[226]\tvalid_0's log_loss: 0.586075\n",
      "[227]\tvalid_0's log_loss: 0.586151\n",
      "[228]\tvalid_0's log_loss: 0.586016\n",
      "[229]\tvalid_0's log_loss: 0.586234\n",
      "[230]\tvalid_0's log_loss: 0.586201\n",
      "[231]\tvalid_0's log_loss: 0.586159\n",
      "[232]\tvalid_0's log_loss: 0.585817\n",
      "[233]\tvalid_0's log_loss: 0.58586\n",
      "[234]\tvalid_0's log_loss: 0.586031\n",
      "[235]\tvalid_0's log_loss: 0.585866\n",
      "[236]\tvalid_0's log_loss: 0.585897\n",
      "[237]\tvalid_0's log_loss: 0.585989\n",
      "[238]\tvalid_0's log_loss: 0.586209\n",
      "[239]\tvalid_0's log_loss: 0.586531\n",
      "[240]\tvalid_0's log_loss: 0.586832\n",
      "[241]\tvalid_0's log_loss: 0.587145\n",
      "[242]\tvalid_0's log_loss: 0.587222\n",
      "[243]\tvalid_0's log_loss: 0.587323\n",
      "[244]\tvalid_0's log_loss: 0.587006\n",
      "[245]\tvalid_0's log_loss: 0.587349\n",
      "[246]\tvalid_0's log_loss: 0.587283\n",
      "[247]\tvalid_0's log_loss: 0.587466\n",
      "[248]\tvalid_0's log_loss: 0.587779\n",
      "[249]\tvalid_0's log_loss: 0.587911\n",
      "[250]\tvalid_0's log_loss: 0.587873\n",
      "[251]\tvalid_0's log_loss: 0.587703\n",
      "[252]\tvalid_0's log_loss: 0.587994\n",
      "[253]\tvalid_0's log_loss: 0.587861\n",
      "[254]\tvalid_0's log_loss: 0.588062\n",
      "[255]\tvalid_0's log_loss: 0.588073\n",
      "[256]\tvalid_0's log_loss: 0.588242\n",
      "[257]\tvalid_0's log_loss: 0.588237\n",
      "[258]\tvalid_0's log_loss: 0.588492\n",
      "[259]\tvalid_0's log_loss: 0.588506\n",
      "[260]\tvalid_0's log_loss: 0.588733\n",
      "[261]\tvalid_0's log_loss: 0.588775\n",
      "[262]\tvalid_0's log_loss: 0.588748\n",
      "[263]\tvalid_0's log_loss: 0.588707\n",
      "[264]\tvalid_0's log_loss: 0.58942\n",
      "[265]\tvalid_0's log_loss: 0.589494\n",
      "[266]\tvalid_0's log_loss: 0.589479\n",
      "[267]\tvalid_0's log_loss: 0.589474\n",
      "[268]\tvalid_0's log_loss: 0.589262\n",
      "[269]\tvalid_0's log_loss: 0.589168\n",
      "[270]\tvalid_0's log_loss: 0.58933\n",
      "[271]\tvalid_0's log_loss: 0.589593\n",
      "[272]\tvalid_0's log_loss: 0.58939\n",
      "[273]\tvalid_0's log_loss: 0.589213\n",
      "[274]\tvalid_0's log_loss: 0.589043\n",
      "[275]\tvalid_0's log_loss: 0.58889\n",
      "[276]\tvalid_0's log_loss: 0.58912\n",
      "[277]\tvalid_0's log_loss: 0.589282\n",
      "[278]\tvalid_0's log_loss: 0.589113\n",
      "[279]\tvalid_0's log_loss: 0.589032\n",
      "[280]\tvalid_0's log_loss: 0.589168\n",
      "[281]\tvalid_0's log_loss: 0.588923\n",
      "[282]\tvalid_0's log_loss: 0.588834\n",
      "[283]\tvalid_0's log_loss: 0.589029\n",
      "[284]\tvalid_0's log_loss: 0.589077\n",
      "[285]\tvalid_0's log_loss: 0.589446\n",
      "[286]\tvalid_0's log_loss: 0.589668\n",
      "[287]\tvalid_0's log_loss: 0.589687\n",
      "[288]\tvalid_0's log_loss: 0.590098\n",
      "[289]\tvalid_0's log_loss: 0.590085\n",
      "[290]\tvalid_0's log_loss: 0.590182\n",
      "[291]\tvalid_0's log_loss: 0.590407\n",
      "[292]\tvalid_0's log_loss: 0.590537\n",
      "[293]\tvalid_0's log_loss: 0.590366\n",
      "[294]\tvalid_0's log_loss: 0.590384\n",
      "[295]\tvalid_0's log_loss: 0.590021\n",
      "[296]\tvalid_0's log_loss: 0.590061\n",
      "[297]\tvalid_0's log_loss: 0.590126\n",
      "[298]\tvalid_0's log_loss: 0.589997\n",
      "[299]\tvalid_0's log_loss: 0.590301\n",
      "[300]\tvalid_0's log_loss: 0.590453\n",
      "[301]\tvalid_0's log_loss: 0.590464\n",
      "[302]\tvalid_0's log_loss: 0.590328\n",
      "[303]\tvalid_0's log_loss: 0.590438\n",
      "[304]\tvalid_0's log_loss: 0.590606\n",
      "[305]\tvalid_0's log_loss: 0.590671\n",
      "[306]\tvalid_0's log_loss: 0.590763\n",
      "[307]\tvalid_0's log_loss: 0.591001\n",
      "[308]\tvalid_0's log_loss: 0.591107\n",
      "[309]\tvalid_0's log_loss: 0.591008\n",
      "[310]\tvalid_0's log_loss: 0.59102\n",
      "[311]\tvalid_0's log_loss: 0.59123\n",
      "[312]\tvalid_0's log_loss: 0.591295\n",
      "[313]\tvalid_0's log_loss: 0.591554\n",
      "[314]\tvalid_0's log_loss: 0.591398\n",
      "[315]\tvalid_0's log_loss: 0.591259\n",
      "[316]\tvalid_0's log_loss: 0.591406\n",
      "[317]\tvalid_0's log_loss: 0.591662\n",
      "[318]\tvalid_0's log_loss: 0.591647\n",
      "[319]\tvalid_0's log_loss: 0.591446\n",
      "[320]\tvalid_0's log_loss: 0.591353\n",
      "[321]\tvalid_0's log_loss: 0.591439\n",
      "[322]\tvalid_0's log_loss: 0.591705\n",
      "[323]\tvalid_0's log_loss: 0.59171\n",
      "[324]\tvalid_0's log_loss: 0.591952\n",
      "[325]\tvalid_0's log_loss: 0.592155\n",
      "[326]\tvalid_0's log_loss: 0.592212\n",
      "[327]\tvalid_0's log_loss: 0.592226\n",
      "[328]\tvalid_0's log_loss: 0.592303\n",
      "[329]\tvalid_0's log_loss: 0.592541\n",
      "[330]\tvalid_0's log_loss: 0.592565\n",
      "[331]\tvalid_0's log_loss: 0.592379\n",
      "[332]\tvalid_0's log_loss: 0.592557\n",
      "[333]\tvalid_0's log_loss: 0.592577\n",
      "[334]\tvalid_0's log_loss: 0.592701\n",
      "[335]\tvalid_0's log_loss: 0.593101\n",
      "[336]\tvalid_0's log_loss: 0.593039\n",
      "[337]\tvalid_0's log_loss: 0.592923\n",
      "[338]\tvalid_0's log_loss: 0.592966\n",
      "[339]\tvalid_0's log_loss: 0.593181\n",
      "[340]\tvalid_0's log_loss: 0.593205\n",
      "[341]\tvalid_0's log_loss: 0.593328\n",
      "[342]\tvalid_0's log_loss: 0.59317\n",
      "[343]\tvalid_0's log_loss: 0.593009\n",
      "[344]\tvalid_0's log_loss: 0.593083\n",
      "[345]\tvalid_0's log_loss: 0.593159\n",
      "[346]\tvalid_0's log_loss: 0.593138\n",
      "[347]\tvalid_0's log_loss: 0.593256\n",
      "[348]\tvalid_0's log_loss: 0.593356\n",
      "[349]\tvalid_0's log_loss: 0.593537\n",
      "[350]\tvalid_0's log_loss: 0.593665\n",
      "[351]\tvalid_0's log_loss: 0.59369\n",
      "[352]\tvalid_0's log_loss: 0.594081\n",
      "[353]\tvalid_0's log_loss: 0.594072\n",
      "[354]\tvalid_0's log_loss: 0.594275\n",
      "[355]\tvalid_0's log_loss: 0.594191\n",
      "[356]\tvalid_0's log_loss: 0.594019\n",
      "[357]\tvalid_0's log_loss: 0.593883\n",
      "[358]\tvalid_0's log_loss: 0.594186\n",
      "[359]\tvalid_0's log_loss: 0.594166\n",
      "[360]\tvalid_0's log_loss: 0.594482\n",
      "[361]\tvalid_0's log_loss: 0.59443\n",
      "[362]\tvalid_0's log_loss: 0.59453\n",
      "[363]\tvalid_0's log_loss: 0.594759\n",
      "[364]\tvalid_0's log_loss: 0.594776\n",
      "[365]\tvalid_0's log_loss: 0.594748\n",
      "[366]\tvalid_0's log_loss: 0.594383\n",
      "[367]\tvalid_0's log_loss: 0.594816\n",
      "[368]\tvalid_0's log_loss: 0.595094\n",
      "[369]\tvalid_0's log_loss: 0.59481\n",
      "[370]\tvalid_0's log_loss: 0.594956\n",
      "[371]\tvalid_0's log_loss: 0.595012\n",
      "[372]\tvalid_0's log_loss: 0.595422\n",
      "[373]\tvalid_0's log_loss: 0.595344\n",
      "[374]\tvalid_0's log_loss: 0.59542\n",
      "[375]\tvalid_0's log_loss: 0.595357\n",
      "[376]\tvalid_0's log_loss: 0.595338\n",
      "[377]\tvalid_0's log_loss: 0.59558\n",
      "[378]\tvalid_0's log_loss: 0.595659\n",
      "[379]\tvalid_0's log_loss: 0.59568\n",
      "[380]\tvalid_0's log_loss: 0.596103\n",
      "[381]\tvalid_0's log_loss: 0.596189\n",
      "[382]\tvalid_0's log_loss: 0.596034\n",
      "[383]\tvalid_0's log_loss: 0.595934\n",
      "[384]\tvalid_0's log_loss: 0.596201\n",
      "[385]\tvalid_0's log_loss: 0.596507\n",
      "[386]\tvalid_0's log_loss: 0.596596\n",
      "[387]\tvalid_0's log_loss: 0.596836\n",
      "[388]\tvalid_0's log_loss: 0.597147\n",
      "[389]\tvalid_0's log_loss: 0.597318\n",
      "[390]\tvalid_0's log_loss: 0.597271\n",
      "[391]\tvalid_0's log_loss: 0.597227\n",
      "[392]\tvalid_0's log_loss: 0.597323\n",
      "[393]\tvalid_0's log_loss: 0.597611\n",
      "[394]\tvalid_0's log_loss: 0.597551\n",
      "[395]\tvalid_0's log_loss: 0.597627\n",
      "[396]\tvalid_0's log_loss: 0.597627\n",
      "[397]\tvalid_0's log_loss: 0.598081\n",
      "[398]\tvalid_0's log_loss: 0.598102\n",
      "[399]\tvalid_0's log_loss: 0.598065\n",
      "[400]\tvalid_0's log_loss: 0.597972\n",
      "[401]\tvalid_0's log_loss: 0.598134\n",
      "[402]\tvalid_0's log_loss: 0.598518\n",
      "[403]\tvalid_0's log_loss: 0.598427\n",
      "[404]\tvalid_0's log_loss: 0.598569\n",
      "[405]\tvalid_0's log_loss: 0.598875\n",
      "[406]\tvalid_0's log_loss: 0.598928\n",
      "[407]\tvalid_0's log_loss: 0.599243\n",
      "[408]\tvalid_0's log_loss: 0.599155\n",
      "[409]\tvalid_0's log_loss: 0.599166\n",
      "[410]\tvalid_0's log_loss: 0.598741\n",
      "[411]\tvalid_0's log_loss: 0.598933\n",
      "[412]\tvalid_0's log_loss: 0.599101\n",
      "[413]\tvalid_0's log_loss: 0.599173\n",
      "[414]\tvalid_0's log_loss: 0.599148\n",
      "[415]\tvalid_0's log_loss: 0.599288\n",
      "[416]\tvalid_0's log_loss: 0.59925\n",
      "[417]\tvalid_0's log_loss: 0.599373\n",
      "[418]\tvalid_0's log_loss: 0.599414\n",
      "[419]\tvalid_0's log_loss: 0.599715\n",
      "[420]\tvalid_0's log_loss: 0.599794\n",
      "Early stopping, best iteration is:\n",
      "[120]\tvalid_0's log_loss: 0.581668\n",
      "[1]\tvalid_0's log_loss: 0.555819\n",
      "Train until valid scores didn't improve in 300 rounds.\n",
      "[2]\tvalid_0's log_loss: 0.55611\n",
      "[3]\tvalid_0's log_loss: 0.556129\n",
      "[4]\tvalid_0's log_loss: 0.556667\n",
      "[5]\tvalid_0's log_loss: 0.556086\n",
      "[6]\tvalid_0's log_loss: 0.556388\n",
      "[7]\tvalid_0's log_loss: 0.556342\n",
      "[8]\tvalid_0's log_loss: 0.5561\n",
      "[9]\tvalid_0's log_loss: 0.555943\n",
      "[10]\tvalid_0's log_loss: 0.555369\n",
      "[11]\tvalid_0's log_loss: 0.555164\n",
      "[12]\tvalid_0's log_loss: 0.555207\n",
      "[13]\tvalid_0's log_loss: 0.555227\n",
      "[14]\tvalid_0's log_loss: 0.555057\n",
      "[15]\tvalid_0's log_loss: 0.555146\n",
      "[16]\tvalid_0's log_loss: 0.554876\n",
      "[17]\tvalid_0's log_loss: 0.554492\n",
      "[18]\tvalid_0's log_loss: 0.554648\n",
      "[19]\tvalid_0's log_loss: 0.555128\n",
      "[20]\tvalid_0's log_loss: 0.554467\n",
      "[21]\tvalid_0's log_loss: 0.554412\n",
      "[22]\tvalid_0's log_loss: 0.554638\n",
      "[23]\tvalid_0's log_loss: 0.554953\n",
      "[24]\tvalid_0's log_loss: 0.554947\n",
      "[25]\tvalid_0's log_loss: 0.555087\n",
      "[26]\tvalid_0's log_loss: 0.554771\n",
      "[27]\tvalid_0's log_loss: 0.554312\n",
      "[28]\tvalid_0's log_loss: 0.554388\n",
      "[29]\tvalid_0's log_loss: 0.554644\n",
      "[30]\tvalid_0's log_loss: 0.554834\n",
      "[31]\tvalid_0's log_loss: 0.55487\n",
      "[32]\tvalid_0's log_loss: 0.554782\n",
      "[33]\tvalid_0's log_loss: 0.554605\n",
      "[34]\tvalid_0's log_loss: 0.553557\n",
      "[35]\tvalid_0's log_loss: 0.553208\n",
      "[36]\tvalid_0's log_loss: 0.5529\n",
      "[37]\tvalid_0's log_loss: 0.55272\n",
      "[38]\tvalid_0's log_loss: 0.55282\n",
      "[39]\tvalid_0's log_loss: 0.552914\n",
      "[40]\tvalid_0's log_loss: 0.552852\n",
      "[41]\tvalid_0's log_loss: 0.552762\n",
      "[42]\tvalid_0's log_loss: 0.552851\n",
      "[43]\tvalid_0's log_loss: 0.55266\n",
      "[44]\tvalid_0's log_loss: 0.552485\n",
      "[45]\tvalid_0's log_loss: 0.552508\n",
      "[46]\tvalid_0's log_loss: 0.552252\n",
      "[47]\tvalid_0's log_loss: 0.552334\n",
      "[48]\tvalid_0's log_loss: 0.552042\n",
      "[49]\tvalid_0's log_loss: 0.552437\n",
      "[50]\tvalid_0's log_loss: 0.552428\n",
      "[51]\tvalid_0's log_loss: 0.552553\n",
      "[52]\tvalid_0's log_loss: 0.552356\n",
      "[53]\tvalid_0's log_loss: 0.552316\n",
      "[54]\tvalid_0's log_loss: 0.552251\n",
      "[55]\tvalid_0's log_loss: 0.552148\n",
      "[56]\tvalid_0's log_loss: 0.552483\n",
      "[57]\tvalid_0's log_loss: 0.552047\n",
      "[58]\tvalid_0's log_loss: 0.55194\n",
      "[59]\tvalid_0's log_loss: 0.551908\n",
      "[60]\tvalid_0's log_loss: 0.551884\n",
      "[61]\tvalid_0's log_loss: 0.552074\n",
      "[62]\tvalid_0's log_loss: 0.551741\n",
      "[63]\tvalid_0's log_loss: 0.551832\n",
      "[64]\tvalid_0's log_loss: 0.551367\n",
      "[65]\tvalid_0's log_loss: 0.552026\n",
      "[66]\tvalid_0's log_loss: 0.55194\n",
      "[67]\tvalid_0's log_loss: 0.551967\n",
      "[68]\tvalid_0's log_loss: 0.551863\n",
      "[69]\tvalid_0's log_loss: 0.551987\n",
      "[70]\tvalid_0's log_loss: 0.551977\n",
      "[71]\tvalid_0's log_loss: 0.551993\n",
      "[72]\tvalid_0's log_loss: 0.552108\n",
      "[73]\tvalid_0's log_loss: 0.551946\n",
      "[74]\tvalid_0's log_loss: 0.552065\n",
      "[75]\tvalid_0's log_loss: 0.552528\n",
      "[76]\tvalid_0's log_loss: 0.55301\n",
      "[77]\tvalid_0's log_loss: 0.552686\n",
      "[78]\tvalid_0's log_loss: 0.552638\n",
      "[79]\tvalid_0's log_loss: 0.552235\n",
      "[80]\tvalid_0's log_loss: 0.552379\n",
      "[81]\tvalid_0's log_loss: 0.552529\n",
      "[82]\tvalid_0's log_loss: 0.552314\n",
      "[83]\tvalid_0's log_loss: 0.552263\n",
      "[84]\tvalid_0's log_loss: 0.552227\n",
      "[85]\tvalid_0's log_loss: 0.552346\n",
      "[86]\tvalid_0's log_loss: 0.552602\n",
      "[87]\tvalid_0's log_loss: 0.552441\n",
      "[88]\tvalid_0's log_loss: 0.552353\n",
      "[89]\tvalid_0's log_loss: 0.552149\n",
      "[90]\tvalid_0's log_loss: 0.552207\n",
      "[91]\tvalid_0's log_loss: 0.552311\n",
      "[92]\tvalid_0's log_loss: 0.55227\n",
      "[93]\tvalid_0's log_loss: 0.552602\n",
      "[94]\tvalid_0's log_loss: 0.552453\n",
      "[95]\tvalid_0's log_loss: 0.552412\n",
      "[96]\tvalid_0's log_loss: 0.552452\n",
      "[97]\tvalid_0's log_loss: 0.552361\n",
      "[98]\tvalid_0's log_loss: 0.552064\n",
      "[99]\tvalid_0's log_loss: 0.552382\n",
      "[100]\tvalid_0's log_loss: 0.552136\n",
      "[101]\tvalid_0's log_loss: 0.552063\n",
      "[102]\tvalid_0's log_loss: 0.551955\n",
      "[103]\tvalid_0's log_loss: 0.552048\n",
      "[104]\tvalid_0's log_loss: 0.552176\n",
      "[105]\tvalid_0's log_loss: 0.552031\n",
      "[106]\tvalid_0's log_loss: 0.551942\n",
      "[107]\tvalid_0's log_loss: 0.552007\n",
      "[108]\tvalid_0's log_loss: 0.552071\n",
      "[109]\tvalid_0's log_loss: 0.552352\n",
      "[110]\tvalid_0's log_loss: 0.552518\n",
      "[111]\tvalid_0's log_loss: 0.55271\n",
      "[112]\tvalid_0's log_loss: 0.552498\n",
      "[113]\tvalid_0's log_loss: 0.552363\n",
      "[114]\tvalid_0's log_loss: 0.552351\n",
      "[115]\tvalid_0's log_loss: 0.552646\n",
      "[116]\tvalid_0's log_loss: 0.552408\n",
      "[117]\tvalid_0's log_loss: 0.552403\n",
      "[118]\tvalid_0's log_loss: 0.55243\n",
      "[119]\tvalid_0's log_loss: 0.552273\n",
      "[120]\tvalid_0's log_loss: 0.552448\n",
      "[121]\tvalid_0's log_loss: 0.552575\n",
      "[122]\tvalid_0's log_loss: 0.552447\n",
      "[123]\tvalid_0's log_loss: 0.552639\n",
      "[124]\tvalid_0's log_loss: 0.552405\n",
      "[125]\tvalid_0's log_loss: 0.552323\n",
      "[126]\tvalid_0's log_loss: 0.552219\n",
      "[127]\tvalid_0's log_loss: 0.552283\n",
      "[128]\tvalid_0's log_loss: 0.552561\n",
      "[129]\tvalid_0's log_loss: 0.552778\n",
      "[130]\tvalid_0's log_loss: 0.552632\n",
      "[131]\tvalid_0's log_loss: 0.552616\n",
      "[132]\tvalid_0's log_loss: 0.552578\n",
      "[133]\tvalid_0's log_loss: 0.552682\n",
      "[134]\tvalid_0's log_loss: 0.552775\n",
      "[135]\tvalid_0's log_loss: 0.552929\n",
      "[136]\tvalid_0's log_loss: 0.552906\n",
      "[137]\tvalid_0's log_loss: 0.55258\n",
      "[138]\tvalid_0's log_loss: 0.552649\n",
      "[139]\tvalid_0's log_loss: 0.552716\n",
      "[140]\tvalid_0's log_loss: 0.552889\n",
      "[141]\tvalid_0's log_loss: 0.552987\n",
      "[142]\tvalid_0's log_loss: 0.553012\n",
      "[143]\tvalid_0's log_loss: 0.552632\n",
      "[144]\tvalid_0's log_loss: 0.552611\n",
      "[145]\tvalid_0's log_loss: 0.552307\n",
      "[146]\tvalid_0's log_loss: 0.552628\n",
      "[147]\tvalid_0's log_loss: 0.55257\n",
      "[148]\tvalid_0's log_loss: 0.552387\n",
      "[149]\tvalid_0's log_loss: 0.551604\n",
      "[150]\tvalid_0's log_loss: 0.55168\n",
      "[151]\tvalid_0's log_loss: 0.551797\n",
      "[152]\tvalid_0's log_loss: 0.551858\n",
      "[153]\tvalid_0's log_loss: 0.551885\n",
      "[154]\tvalid_0's log_loss: 0.55177\n",
      "[155]\tvalid_0's log_loss: 0.551937\n",
      "[156]\tvalid_0's log_loss: 0.552007\n",
      "[157]\tvalid_0's log_loss: 0.552118\n",
      "[158]\tvalid_0's log_loss: 0.552276\n",
      "[159]\tvalid_0's log_loss: 0.552414\n",
      "[160]\tvalid_0's log_loss: 0.552383\n",
      "[161]\tvalid_0's log_loss: 0.552611\n",
      "[162]\tvalid_0's log_loss: 0.552593\n",
      "[163]\tvalid_0's log_loss: 0.552447\n",
      "[164]\tvalid_0's log_loss: 0.552266\n",
      "[165]\tvalid_0's log_loss: 0.551882\n",
      "[166]\tvalid_0's log_loss: 0.552052\n",
      "[167]\tvalid_0's log_loss: 0.551974\n",
      "[168]\tvalid_0's log_loss: 0.551908\n",
      "[169]\tvalid_0's log_loss: 0.552035\n",
      "[170]\tvalid_0's log_loss: 0.551774\n",
      "[171]\tvalid_0's log_loss: 0.551742\n",
      "[172]\tvalid_0's log_loss: 0.551832\n",
      "[173]\tvalid_0's log_loss: 0.551991\n",
      "[174]\tvalid_0's log_loss: 0.55188\n",
      "[175]\tvalid_0's log_loss: 0.551514\n",
      "[176]\tvalid_0's log_loss: 0.551281\n",
      "[177]\tvalid_0's log_loss: 0.551289\n",
      "[178]\tvalid_0's log_loss: 0.551251\n",
      "[179]\tvalid_0's log_loss: 0.551676\n",
      "[180]\tvalid_0's log_loss: 0.551687\n",
      "[181]\tvalid_0's log_loss: 0.551716\n",
      "[182]\tvalid_0's log_loss: 0.551688\n",
      "[183]\tvalid_0's log_loss: 0.55197\n",
      "[184]\tvalid_0's log_loss: 0.552016\n",
      "[185]\tvalid_0's log_loss: 0.55186\n",
      "[186]\tvalid_0's log_loss: 0.551629\n",
      "[187]\tvalid_0's log_loss: 0.551695\n",
      "[188]\tvalid_0's log_loss: 0.551732\n",
      "[189]\tvalid_0's log_loss: 0.551907\n",
      "[190]\tvalid_0's log_loss: 0.552073\n",
      "[191]\tvalid_0's log_loss: 0.552127\n",
      "[192]\tvalid_0's log_loss: 0.55239\n",
      "[193]\tvalid_0's log_loss: 0.552603\n",
      "[194]\tvalid_0's log_loss: 0.55272\n",
      "[195]\tvalid_0's log_loss: 0.552644\n",
      "[196]\tvalid_0's log_loss: 0.552672\n",
      "[197]\tvalid_0's log_loss: 0.552897\n",
      "[198]\tvalid_0's log_loss: 0.552762\n",
      "[199]\tvalid_0's log_loss: 0.552998\n",
      "[200]\tvalid_0's log_loss: 0.553377\n",
      "[201]\tvalid_0's log_loss: 0.553396\n",
      "[202]\tvalid_0's log_loss: 0.553547\n",
      "[203]\tvalid_0's log_loss: 0.553684\n",
      "[204]\tvalid_0's log_loss: 0.553772\n",
      "[205]\tvalid_0's log_loss: 0.553839\n",
      "[206]\tvalid_0's log_loss: 0.554064\n",
      "[207]\tvalid_0's log_loss: 0.554222\n",
      "[208]\tvalid_0's log_loss: 0.554053\n",
      "[209]\tvalid_0's log_loss: 0.554175\n",
      "[210]\tvalid_0's log_loss: 0.553776\n",
      "[211]\tvalid_0's log_loss: 0.554079\n",
      "[212]\tvalid_0's log_loss: 0.554336\n",
      "[213]\tvalid_0's log_loss: 0.554238\n",
      "[214]\tvalid_0's log_loss: 0.554052\n",
      "[215]\tvalid_0's log_loss: 0.554072\n",
      "[216]\tvalid_0's log_loss: 0.553634\n",
      "[217]\tvalid_0's log_loss: 0.553641\n",
      "[218]\tvalid_0's log_loss: 0.554043\n",
      "[219]\tvalid_0's log_loss: 0.553817\n",
      "[220]\tvalid_0's log_loss: 0.553462\n",
      "[221]\tvalid_0's log_loss: 0.553635\n",
      "[222]\tvalid_0's log_loss: 0.553722\n",
      "[223]\tvalid_0's log_loss: 0.553838\n",
      "[224]\tvalid_0's log_loss: 0.553933\n",
      "[225]\tvalid_0's log_loss: 0.55412\n",
      "[226]\tvalid_0's log_loss: 0.554181\n",
      "[227]\tvalid_0's log_loss: 0.55416\n",
      "[228]\tvalid_0's log_loss: 0.554299\n",
      "[229]\tvalid_0's log_loss: 0.554649\n",
      "[230]\tvalid_0's log_loss: 0.555047\n",
      "[231]\tvalid_0's log_loss: 0.555195\n",
      "[232]\tvalid_0's log_loss: 0.555099\n",
      "[233]\tvalid_0's log_loss: 0.554733\n",
      "[234]\tvalid_0's log_loss: 0.55491\n",
      "[235]\tvalid_0's log_loss: 0.55509\n",
      "[236]\tvalid_0's log_loss: 0.555028\n",
      "[237]\tvalid_0's log_loss: 0.555249\n",
      "[238]\tvalid_0's log_loss: 0.555154\n",
      "[239]\tvalid_0's log_loss: 0.555039\n",
      "[240]\tvalid_0's log_loss: 0.555164\n",
      "[241]\tvalid_0's log_loss: 0.555245\n",
      "[242]\tvalid_0's log_loss: 0.555206\n",
      "[243]\tvalid_0's log_loss: 0.555227\n",
      "[244]\tvalid_0's log_loss: 0.555306\n",
      "[245]\tvalid_0's log_loss: 0.555705\n",
      "[246]\tvalid_0's log_loss: 0.555817\n",
      "[247]\tvalid_0's log_loss: 0.555839\n",
      "[248]\tvalid_0's log_loss: 0.555985\n",
      "[249]\tvalid_0's log_loss: 0.555782\n",
      "[250]\tvalid_0's log_loss: 0.55597\n",
      "[251]\tvalid_0's log_loss: 0.556153\n",
      "[252]\tvalid_0's log_loss: 0.556477\n",
      "[253]\tvalid_0's log_loss: 0.556583\n",
      "[254]\tvalid_0's log_loss: 0.556493\n",
      "[255]\tvalid_0's log_loss: 0.556486\n",
      "[256]\tvalid_0's log_loss: 0.556514\n",
      "[257]\tvalid_0's log_loss: 0.556661\n",
      "[258]\tvalid_0's log_loss: 0.556727\n",
      "[259]\tvalid_0's log_loss: 0.556918\n",
      "[260]\tvalid_0's log_loss: 0.556911\n",
      "[261]\tvalid_0's log_loss: 0.557233\n",
      "[262]\tvalid_0's log_loss: 0.557438\n",
      "[263]\tvalid_0's log_loss: 0.557349\n",
      "[264]\tvalid_0's log_loss: 0.557371\n",
      "[265]\tvalid_0's log_loss: 0.557224\n",
      "[266]\tvalid_0's log_loss: 0.55718\n",
      "[267]\tvalid_0's log_loss: 0.557105\n",
      "[268]\tvalid_0's log_loss: 0.557411\n",
      "[269]\tvalid_0's log_loss: 0.557464\n",
      "[270]\tvalid_0's log_loss: 0.557227\n",
      "[271]\tvalid_0's log_loss: 0.557445\n",
      "[272]\tvalid_0's log_loss: 0.557356\n",
      "[273]\tvalid_0's log_loss: 0.557109\n",
      "[274]\tvalid_0's log_loss: 0.55723\n",
      "[275]\tvalid_0's log_loss: 0.557309\n",
      "[276]\tvalid_0's log_loss: 0.557401\n",
      "[277]\tvalid_0's log_loss: 0.557602\n",
      "[278]\tvalid_0's log_loss: 0.557356\n",
      "[279]\tvalid_0's log_loss: 0.55739\n",
      "[280]\tvalid_0's log_loss: 0.557494\n",
      "[281]\tvalid_0's log_loss: 0.557482\n",
      "[282]\tvalid_0's log_loss: 0.557336\n",
      "[283]\tvalid_0's log_loss: 0.557291\n",
      "[284]\tvalid_0's log_loss: 0.557285\n",
      "[285]\tvalid_0's log_loss: 0.55706\n",
      "[286]\tvalid_0's log_loss: 0.557005\n",
      "[287]\tvalid_0's log_loss: 0.557083\n",
      "[288]\tvalid_0's log_loss: 0.556928\n",
      "[289]\tvalid_0's log_loss: 0.556702\n",
      "[290]\tvalid_0's log_loss: 0.556679\n",
      "[291]\tvalid_0's log_loss: 0.556874\n",
      "[292]\tvalid_0's log_loss: 0.556835\n",
      "[293]\tvalid_0's log_loss: 0.556799\n",
      "[294]\tvalid_0's log_loss: 0.556887\n",
      "[295]\tvalid_0's log_loss: 0.557088\n",
      "[296]\tvalid_0's log_loss: 0.556934\n",
      "[297]\tvalid_0's log_loss: 0.557082\n",
      "[298]\tvalid_0's log_loss: 0.557128\n",
      "[299]\tvalid_0's log_loss: 0.557672\n",
      "[300]\tvalid_0's log_loss: 0.557793\n",
      "[301]\tvalid_0's log_loss: 0.557971\n",
      "[302]\tvalid_0's log_loss: 0.557706\n",
      "[303]\tvalid_0's log_loss: 0.557987\n",
      "[304]\tvalid_0's log_loss: 0.558033\n",
      "[305]\tvalid_0's log_loss: 0.557922\n",
      "[306]\tvalid_0's log_loss: 0.557757\n",
      "[307]\tvalid_0's log_loss: 0.558013\n",
      "[308]\tvalid_0's log_loss: 0.558065\n",
      "[309]\tvalid_0's log_loss: 0.558002\n",
      "[310]\tvalid_0's log_loss: 0.55766\n",
      "[311]\tvalid_0's log_loss: 0.557831\n",
      "[312]\tvalid_0's log_loss: 0.557892\n",
      "[313]\tvalid_0's log_loss: 0.558\n",
      "[314]\tvalid_0's log_loss: 0.558144\n",
      "[315]\tvalid_0's log_loss: 0.558155\n",
      "[316]\tvalid_0's log_loss: 0.558491\n",
      "[317]\tvalid_0's log_loss: 0.558617\n",
      "[318]\tvalid_0's log_loss: 0.558416\n",
      "[319]\tvalid_0's log_loss: 0.558537\n",
      "[320]\tvalid_0's log_loss: 0.558662\n",
      "[321]\tvalid_0's log_loss: 0.558715\n",
      "[322]\tvalid_0's log_loss: 0.558798\n",
      "[323]\tvalid_0's log_loss: 0.559075\n",
      "[324]\tvalid_0's log_loss: 0.559001\n",
      "[325]\tvalid_0's log_loss: 0.559201\n",
      "[326]\tvalid_0's log_loss: 0.559117\n",
      "[327]\tvalid_0's log_loss: 0.559173\n",
      "[328]\tvalid_0's log_loss: 0.559322\n",
      "[329]\tvalid_0's log_loss: 0.559323\n",
      "[330]\tvalid_0's log_loss: 0.5593\n",
      "[331]\tvalid_0's log_loss: 0.559201\n",
      "[332]\tvalid_0's log_loss: 0.559279\n",
      "[333]\tvalid_0's log_loss: 0.55947\n",
      "[334]\tvalid_0's log_loss: 0.559268\n",
      "[335]\tvalid_0's log_loss: 0.559378\n",
      "[336]\tvalid_0's log_loss: 0.559342\n",
      "[337]\tvalid_0's log_loss: 0.559607\n",
      "[338]\tvalid_0's log_loss: 0.559323\n",
      "[339]\tvalid_0's log_loss: 0.559233\n",
      "[340]\tvalid_0's log_loss: 0.559109\n",
      "[341]\tvalid_0's log_loss: 0.559215\n",
      "[342]\tvalid_0's log_loss: 0.559305\n",
      "[343]\tvalid_0's log_loss: 0.55917\n",
      "[344]\tvalid_0's log_loss: 0.559329\n",
      "[345]\tvalid_0's log_loss: 0.559307\n",
      "[346]\tvalid_0's log_loss: 0.559421\n",
      "[347]\tvalid_0's log_loss: 0.559465\n",
      "[348]\tvalid_0's log_loss: 0.559279\n",
      "[349]\tvalid_0's log_loss: 0.559046\n",
      "[350]\tvalid_0's log_loss: 0.559008\n",
      "[351]\tvalid_0's log_loss: 0.558972\n",
      "[352]\tvalid_0's log_loss: 0.559044\n",
      "[353]\tvalid_0's log_loss: 0.559026\n",
      "[354]\tvalid_0's log_loss: 0.559116\n",
      "[355]\tvalid_0's log_loss: 0.559139\n",
      "[356]\tvalid_0's log_loss: 0.55931\n",
      "[357]\tvalid_0's log_loss: 0.559156\n",
      "[358]\tvalid_0's log_loss: 0.559213\n",
      "[359]\tvalid_0's log_loss: 0.559191\n",
      "[360]\tvalid_0's log_loss: 0.559332\n",
      "[361]\tvalid_0's log_loss: 0.559205\n",
      "[362]\tvalid_0's log_loss: 0.559527\n",
      "[363]\tvalid_0's log_loss: 0.559526\n",
      "[364]\tvalid_0's log_loss: 0.559594\n",
      "[365]\tvalid_0's log_loss: 0.559647\n",
      "[366]\tvalid_0's log_loss: 0.559652\n",
      "[367]\tvalid_0's log_loss: 0.559677\n",
      "[368]\tvalid_0's log_loss: 0.559591\n",
      "[369]\tvalid_0's log_loss: 0.559619\n",
      "[370]\tvalid_0's log_loss: 0.559513\n",
      "[371]\tvalid_0's log_loss: 0.55963\n",
      "[372]\tvalid_0's log_loss: 0.559984\n",
      "[373]\tvalid_0's log_loss: 0.560064\n",
      "[374]\tvalid_0's log_loss: 0.560147\n",
      "[375]\tvalid_0's log_loss: 0.560193\n",
      "[376]\tvalid_0's log_loss: 0.560219\n",
      "[377]\tvalid_0's log_loss: 0.560249\n",
      "[378]\tvalid_0's log_loss: 0.560495\n",
      "[379]\tvalid_0's log_loss: 0.560444\n",
      "[380]\tvalid_0's log_loss: 0.560651\n",
      "[381]\tvalid_0's log_loss: 0.560706\n",
      "[382]\tvalid_0's log_loss: 0.560467\n",
      "[383]\tvalid_0's log_loss: 0.560596\n",
      "[384]\tvalid_0's log_loss: 0.56104\n",
      "[385]\tvalid_0's log_loss: 0.561197\n",
      "[386]\tvalid_0's log_loss: 0.561129\n",
      "[387]\tvalid_0's log_loss: 0.561088\n",
      "[388]\tvalid_0's log_loss: 0.561222\n",
      "[389]\tvalid_0's log_loss: 0.561168\n",
      "[390]\tvalid_0's log_loss: 0.561289\n",
      "[391]\tvalid_0's log_loss: 0.561303\n",
      "[392]\tvalid_0's log_loss: 0.561411\n",
      "[393]\tvalid_0's log_loss: 0.561032\n",
      "[394]\tvalid_0's log_loss: 0.560952\n",
      "[395]\tvalid_0's log_loss: 0.561327\n",
      "[396]\tvalid_0's log_loss: 0.561149\n",
      "[397]\tvalid_0's log_loss: 0.561322\n",
      "[398]\tvalid_0's log_loss: 0.561381\n",
      "[399]\tvalid_0's log_loss: 0.561261\n",
      "[400]\tvalid_0's log_loss: 0.561297\n",
      "[401]\tvalid_0's log_loss: 0.561305\n",
      "[402]\tvalid_0's log_loss: 0.561276\n",
      "[403]\tvalid_0's log_loss: 0.561552\n",
      "[404]\tvalid_0's log_loss: 0.561482\n",
      "[405]\tvalid_0's log_loss: 0.561194\n",
      "[406]\tvalid_0's log_loss: 0.561407\n",
      "[407]\tvalid_0's log_loss: 0.56147\n",
      "[408]\tvalid_0's log_loss: 0.561688\n",
      "[409]\tvalid_0's log_loss: 0.562062\n",
      "[410]\tvalid_0's log_loss: 0.562192\n",
      "[411]\tvalid_0's log_loss: 0.562524\n",
      "[412]\tvalid_0's log_loss: 0.562533\n",
      "[413]\tvalid_0's log_loss: 0.562625\n",
      "[414]\tvalid_0's log_loss: 0.562535\n",
      "[415]\tvalid_0's log_loss: 0.562467\n",
      "[416]\tvalid_0's log_loss: 0.562396\n",
      "[417]\tvalid_0's log_loss: 0.562497\n",
      "[418]\tvalid_0's log_loss: 0.562556\n",
      "[419]\tvalid_0's log_loss: 0.56255\n",
      "[420]\tvalid_0's log_loss: 0.562735\n",
      "[421]\tvalid_0's log_loss: 0.563036\n",
      "[422]\tvalid_0's log_loss: 0.563235\n",
      "[423]\tvalid_0's log_loss: 0.563426\n",
      "[424]\tvalid_0's log_loss: 0.563576\n",
      "[425]\tvalid_0's log_loss: 0.56369\n",
      "[426]\tvalid_0's log_loss: 0.563481\n",
      "[427]\tvalid_0's log_loss: 0.56359\n",
      "[428]\tvalid_0's log_loss: 0.563469\n",
      "[429]\tvalid_0's log_loss: 0.56357\n",
      "[430]\tvalid_0's log_loss: 0.563393\n",
      "[431]\tvalid_0's log_loss: 0.563464\n",
      "[432]\tvalid_0's log_loss: 0.563603\n",
      "[433]\tvalid_0's log_loss: 0.563533\n",
      "[434]\tvalid_0's log_loss: 0.563697\n",
      "[435]\tvalid_0's log_loss: 0.563728\n",
      "[436]\tvalid_0's log_loss: 0.563868\n",
      "[437]\tvalid_0's log_loss: 0.563836\n",
      "[438]\tvalid_0's log_loss: 0.563913\n",
      "[439]\tvalid_0's log_loss: 0.56403\n",
      "[440]\tvalid_0's log_loss: 0.564176\n",
      "[441]\tvalid_0's log_loss: 0.564237\n",
      "[442]\tvalid_0's log_loss: 0.56407\n",
      "[443]\tvalid_0's log_loss: 0.564133\n",
      "[444]\tvalid_0's log_loss: 0.564168\n",
      "[445]\tvalid_0's log_loss: 0.564157\n",
      "[446]\tvalid_0's log_loss: 0.564369\n",
      "[447]\tvalid_0's log_loss: 0.564588\n",
      "[448]\tvalid_0's log_loss: 0.564694\n",
      "[449]\tvalid_0's log_loss: 0.564537\n",
      "[450]\tvalid_0's log_loss: 0.564535\n",
      "[451]\tvalid_0's log_loss: 0.564479\n",
      "[452]\tvalid_0's log_loss: 0.564435\n",
      "[453]\tvalid_0's log_loss: 0.564302\n",
      "[454]\tvalid_0's log_loss: 0.564126\n",
      "[455]\tvalid_0's log_loss: 0.564182\n",
      "[456]\tvalid_0's log_loss: 0.564485\n",
      "[457]\tvalid_0's log_loss: 0.564584\n",
      "[458]\tvalid_0's log_loss: 0.564553\n",
      "[459]\tvalid_0's log_loss: 0.56495\n",
      "[460]\tvalid_0's log_loss: 0.56505\n",
      "[461]\tvalid_0's log_loss: 0.565035\n",
      "[462]\tvalid_0's log_loss: 0.56514\n",
      "[463]\tvalid_0's log_loss: 0.565093\n",
      "[464]\tvalid_0's log_loss: 0.565177\n",
      "[465]\tvalid_0's log_loss: 0.565181\n",
      "[466]\tvalid_0's log_loss: 0.565243\n",
      "[467]\tvalid_0's log_loss: 0.565284\n",
      "[468]\tvalid_0's log_loss: 0.565429\n",
      "[469]\tvalid_0's log_loss: 0.565089\n",
      "[470]\tvalid_0's log_loss: 0.564987\n",
      "[471]\tvalid_0's log_loss: 0.564936\n",
      "[472]\tvalid_0's log_loss: 0.564977\n",
      "[473]\tvalid_0's log_loss: 0.565041\n",
      "[474]\tvalid_0's log_loss: 0.565054\n",
      "[475]\tvalid_0's log_loss: 0.565282\n",
      "[476]\tvalid_0's log_loss: 0.565634\n",
      "[477]\tvalid_0's log_loss: 0.565671\n",
      "[478]\tvalid_0's log_loss: 0.565663\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's log_loss: 0.551251\n",
      "[1]\tvalid_0's log_loss: 0.593688\n",
      "Train until valid scores didn't improve in 300 rounds.\n",
      "[2]\tvalid_0's log_loss: 0.593248\n",
      "[3]\tvalid_0's log_loss: 0.593011\n",
      "[4]\tvalid_0's log_loss: 0.592487\n",
      "[5]\tvalid_0's log_loss: 0.592423\n",
      "[6]\tvalid_0's log_loss: 0.592258\n",
      "[7]\tvalid_0's log_loss: 0.591462\n",
      "[8]\tvalid_0's log_loss: 0.591217\n",
      "[9]\tvalid_0's log_loss: 0.590932\n",
      "[10]\tvalid_0's log_loss: 0.590967\n",
      "[11]\tvalid_0's log_loss: 0.590733\n",
      "[12]\tvalid_0's log_loss: 0.590985\n",
      "[13]\tvalid_0's log_loss: 0.5914\n",
      "[14]\tvalid_0's log_loss: 0.59119\n",
      "[15]\tvalid_0's log_loss: 0.591093\n",
      "[16]\tvalid_0's log_loss: 0.591131\n",
      "[17]\tvalid_0's log_loss: 0.590923\n",
      "[18]\tvalid_0's log_loss: 0.59119\n",
      "[19]\tvalid_0's log_loss: 0.591134\n",
      "[20]\tvalid_0's log_loss: 0.590902\n",
      "[21]\tvalid_0's log_loss: 0.590227\n",
      "[22]\tvalid_0's log_loss: 0.590661\n",
      "[23]\tvalid_0's log_loss: 0.590509\n",
      "[24]\tvalid_0's log_loss: 0.590059\n",
      "[25]\tvalid_0's log_loss: 0.58944\n",
      "[26]\tvalid_0's log_loss: 0.589185\n",
      "[27]\tvalid_0's log_loss: 0.589026\n",
      "[28]\tvalid_0's log_loss: 0.589009\n",
      "[29]\tvalid_0's log_loss: 0.588805\n",
      "[30]\tvalid_0's log_loss: 0.588205\n",
      "[31]\tvalid_0's log_loss: 0.587821\n",
      "[32]\tvalid_0's log_loss: 0.587968\n",
      "[33]\tvalid_0's log_loss: 0.58842\n",
      "[34]\tvalid_0's log_loss: 0.588118\n",
      "[35]\tvalid_0's log_loss: 0.587838\n",
      "[36]\tvalid_0's log_loss: 0.588396\n",
      "[37]\tvalid_0's log_loss: 0.588696\n",
      "[38]\tvalid_0's log_loss: 0.588676\n",
      "[39]\tvalid_0's log_loss: 0.58864\n",
      "[40]\tvalid_0's log_loss: 0.588978\n",
      "[41]\tvalid_0's log_loss: 0.58876\n",
      "[42]\tvalid_0's log_loss: 0.589172\n",
      "[43]\tvalid_0's log_loss: 0.588822\n",
      "[44]\tvalid_0's log_loss: 0.589017\n",
      "[45]\tvalid_0's log_loss: 0.589025\n",
      "[46]\tvalid_0's log_loss: 0.588345\n",
      "[47]\tvalid_0's log_loss: 0.588328\n",
      "[48]\tvalid_0's log_loss: 0.588219\n",
      "[49]\tvalid_0's log_loss: 0.588815\n",
      "[50]\tvalid_0's log_loss: 0.588379\n",
      "[51]\tvalid_0's log_loss: 0.587542\n",
      "[52]\tvalid_0's log_loss: 0.587369\n",
      "[53]\tvalid_0's log_loss: 0.587453\n",
      "[54]\tvalid_0's log_loss: 0.587283\n",
      "[55]\tvalid_0's log_loss: 0.58746\n",
      "[56]\tvalid_0's log_loss: 0.587565\n",
      "[57]\tvalid_0's log_loss: 0.587992\n",
      "[58]\tvalid_0's log_loss: 0.588533\n",
      "[59]\tvalid_0's log_loss: 0.588653\n",
      "[60]\tvalid_0's log_loss: 0.588226\n",
      "[61]\tvalid_0's log_loss: 0.588015\n",
      "[62]\tvalid_0's log_loss: 0.587819\n",
      "[63]\tvalid_0's log_loss: 0.588254\n",
      "[64]\tvalid_0's log_loss: 0.5886\n",
      "[65]\tvalid_0's log_loss: 0.58826\n",
      "[66]\tvalid_0's log_loss: 0.588282\n",
      "[67]\tvalid_0's log_loss: 0.588126\n",
      "[68]\tvalid_0's log_loss: 0.588399\n",
      "[69]\tvalid_0's log_loss: 0.588789\n",
      "[70]\tvalid_0's log_loss: 0.588974\n",
      "[71]\tvalid_0's log_loss: 0.588939\n",
      "[72]\tvalid_0's log_loss: 0.589011\n",
      "[73]\tvalid_0's log_loss: 0.588757\n",
      "[74]\tvalid_0's log_loss: 0.588871\n",
      "[75]\tvalid_0's log_loss: 0.588554\n",
      "[76]\tvalid_0's log_loss: 0.588502\n",
      "[77]\tvalid_0's log_loss: 0.58865\n",
      "[78]\tvalid_0's log_loss: 0.588681\n",
      "[79]\tvalid_0's log_loss: 0.588049\n",
      "[80]\tvalid_0's log_loss: 0.587867\n",
      "[81]\tvalid_0's log_loss: 0.587476\n",
      "[82]\tvalid_0's log_loss: 0.587505\n",
      "[83]\tvalid_0's log_loss: 0.586921\n",
      "[84]\tvalid_0's log_loss: 0.587198\n",
      "[85]\tvalid_0's log_loss: 0.586914\n",
      "[86]\tvalid_0's log_loss: 0.586936\n",
      "[87]\tvalid_0's log_loss: 0.586819\n",
      "[88]\tvalid_0's log_loss: 0.586489\n",
      "[89]\tvalid_0's log_loss: 0.586472\n",
      "[90]\tvalid_0's log_loss: 0.586273\n",
      "[91]\tvalid_0's log_loss: 0.586285\n",
      "[92]\tvalid_0's log_loss: 0.586323\n",
      "[93]\tvalid_0's log_loss: 0.586102\n",
      "[94]\tvalid_0's log_loss: 0.585793\n",
      "[95]\tvalid_0's log_loss: 0.585701\n",
      "[96]\tvalid_0's log_loss: 0.585783\n",
      "[97]\tvalid_0's log_loss: 0.585829\n",
      "[98]\tvalid_0's log_loss: 0.585579\n",
      "[99]\tvalid_0's log_loss: 0.585487\n",
      "[100]\tvalid_0's log_loss: 0.585528\n",
      "[101]\tvalid_0's log_loss: 0.585616\n",
      "[102]\tvalid_0's log_loss: 0.585084\n",
      "[103]\tvalid_0's log_loss: 0.58502\n",
      "[104]\tvalid_0's log_loss: 0.585041\n",
      "[105]\tvalid_0's log_loss: 0.585247\n",
      "[106]\tvalid_0's log_loss: 0.585313\n",
      "[107]\tvalid_0's log_loss: 0.585053\n",
      "[108]\tvalid_0's log_loss: 0.585186\n",
      "[109]\tvalid_0's log_loss: 0.585165\n",
      "[110]\tvalid_0's log_loss: 0.585142\n",
      "[111]\tvalid_0's log_loss: 0.585042\n",
      "[112]\tvalid_0's log_loss: 0.585348\n",
      "[113]\tvalid_0's log_loss: 0.585046\n",
      "[114]\tvalid_0's log_loss: 0.585163\n",
      "[115]\tvalid_0's log_loss: 0.58508\n",
      "[116]\tvalid_0's log_loss: 0.585441\n",
      "[117]\tvalid_0's log_loss: 0.58532\n",
      "[118]\tvalid_0's log_loss: 0.585324\n",
      "[119]\tvalid_0's log_loss: 0.585435\n",
      "[120]\tvalid_0's log_loss: 0.585469\n",
      "[121]\tvalid_0's log_loss: 0.58548\n",
      "[122]\tvalid_0's log_loss: 0.5857\n",
      "[123]\tvalid_0's log_loss: 0.585414\n",
      "[124]\tvalid_0's log_loss: 0.585377\n",
      "[125]\tvalid_0's log_loss: 0.585537\n",
      "[126]\tvalid_0's log_loss: 0.585174\n",
      "[127]\tvalid_0's log_loss: 0.58545\n",
      "[128]\tvalid_0's log_loss: 0.585623\n",
      "[129]\tvalid_0's log_loss: 0.586057\n",
      "[130]\tvalid_0's log_loss: 0.586077\n",
      "[131]\tvalid_0's log_loss: 0.586446\n",
      "[132]\tvalid_0's log_loss: 0.586445\n",
      "[133]\tvalid_0's log_loss: 0.586342\n",
      "[134]\tvalid_0's log_loss: 0.586053\n",
      "[135]\tvalid_0's log_loss: 0.585974\n",
      "[136]\tvalid_0's log_loss: 0.585967\n",
      "[137]\tvalid_0's log_loss: 0.586115\n",
      "[138]\tvalid_0's log_loss: 0.585623\n",
      "[139]\tvalid_0's log_loss: 0.585858\n",
      "[140]\tvalid_0's log_loss: 0.585508\n",
      "[141]\tvalid_0's log_loss: 0.585244\n",
      "[142]\tvalid_0's log_loss: 0.585204\n",
      "[143]\tvalid_0's log_loss: 0.585095\n",
      "[144]\tvalid_0's log_loss: 0.584733\n",
      "[145]\tvalid_0's log_loss: 0.584495\n",
      "[146]\tvalid_0's log_loss: 0.584541\n",
      "[147]\tvalid_0's log_loss: 0.584256\n",
      "[148]\tvalid_0's log_loss: 0.584084\n",
      "[149]\tvalid_0's log_loss: 0.584196\n",
      "[150]\tvalid_0's log_loss: 0.584222\n",
      "[151]\tvalid_0's log_loss: 0.584363\n",
      "[152]\tvalid_0's log_loss: 0.58442\n",
      "[153]\tvalid_0's log_loss: 0.584524\n",
      "[154]\tvalid_0's log_loss: 0.584862\n",
      "[155]\tvalid_0's log_loss: 0.584849\n",
      "[156]\tvalid_0's log_loss: 0.58482\n",
      "[157]\tvalid_0's log_loss: 0.584983\n",
      "[158]\tvalid_0's log_loss: 0.585045\n",
      "[159]\tvalid_0's log_loss: 0.584987\n",
      "[160]\tvalid_0's log_loss: 0.585214\n",
      "[161]\tvalid_0's log_loss: 0.585227\n",
      "[162]\tvalid_0's log_loss: 0.585188\n",
      "[163]\tvalid_0's log_loss: 0.585234\n",
      "[164]\tvalid_0's log_loss: 0.585509\n",
      "[165]\tvalid_0's log_loss: 0.585311\n",
      "[166]\tvalid_0's log_loss: 0.585294\n",
      "[167]\tvalid_0's log_loss: 0.585652\n",
      "[168]\tvalid_0's log_loss: 0.586147\n",
      "[169]\tvalid_0's log_loss: 0.586345\n",
      "[170]\tvalid_0's log_loss: 0.586303\n",
      "[171]\tvalid_0's log_loss: 0.586138\n",
      "[172]\tvalid_0's log_loss: 0.585761\n",
      "[173]\tvalid_0's log_loss: 0.585483\n",
      "[174]\tvalid_0's log_loss: 0.585742\n",
      "[175]\tvalid_0's log_loss: 0.585632\n",
      "[176]\tvalid_0's log_loss: 0.585784\n",
      "[177]\tvalid_0's log_loss: 0.585743\n",
      "[178]\tvalid_0's log_loss: 0.585599\n",
      "[179]\tvalid_0's log_loss: 0.585447\n",
      "[180]\tvalid_0's log_loss: 0.58518\n",
      "[181]\tvalid_0's log_loss: 0.585035\n",
      "[182]\tvalid_0's log_loss: 0.584868\n",
      "[183]\tvalid_0's log_loss: 0.584755\n",
      "[184]\tvalid_0's log_loss: 0.584876\n",
      "[185]\tvalid_0's log_loss: 0.584799\n",
      "[186]\tvalid_0's log_loss: 0.585138\n",
      "[187]\tvalid_0's log_loss: 0.585332\n",
      "[188]\tvalid_0's log_loss: 0.585506\n",
      "[189]\tvalid_0's log_loss: 0.585338\n",
      "[190]\tvalid_0's log_loss: 0.585274\n",
      "[191]\tvalid_0's log_loss: 0.585038\n",
      "[192]\tvalid_0's log_loss: 0.585571\n",
      "[193]\tvalid_0's log_loss: 0.585652\n",
      "[194]\tvalid_0's log_loss: 0.585457\n",
      "[195]\tvalid_0's log_loss: 0.585463\n",
      "[196]\tvalid_0's log_loss: 0.585579\n",
      "[197]\tvalid_0's log_loss: 0.585614\n",
      "[198]\tvalid_0's log_loss: 0.585906\n",
      "[199]\tvalid_0's log_loss: 0.585801\n",
      "[200]\tvalid_0's log_loss: 0.585853\n",
      "[201]\tvalid_0's log_loss: 0.585752\n",
      "[202]\tvalid_0's log_loss: 0.58563\n",
      "[203]\tvalid_0's log_loss: 0.585567\n",
      "[204]\tvalid_0's log_loss: 0.585201\n",
      "[205]\tvalid_0's log_loss: 0.585225\n",
      "[206]\tvalid_0's log_loss: 0.585007\n",
      "[207]\tvalid_0's log_loss: 0.585096\n",
      "[208]\tvalid_0's log_loss: 0.585214\n",
      "[209]\tvalid_0's log_loss: 0.585578\n",
      "[210]\tvalid_0's log_loss: 0.585517\n",
      "[211]\tvalid_0's log_loss: 0.585449\n",
      "[212]\tvalid_0's log_loss: 0.585326\n",
      "[213]\tvalid_0's log_loss: 0.585274\n",
      "[214]\tvalid_0's log_loss: 0.585501\n",
      "[215]\tvalid_0's log_loss: 0.58574\n",
      "[216]\tvalid_0's log_loss: 0.585745\n",
      "[217]\tvalid_0's log_loss: 0.585623\n",
      "[218]\tvalid_0's log_loss: 0.585419\n",
      "[219]\tvalid_0's log_loss: 0.585384\n",
      "[220]\tvalid_0's log_loss: 0.585361\n",
      "[221]\tvalid_0's log_loss: 0.585416\n",
      "[222]\tvalid_0's log_loss: 0.585368\n",
      "[223]\tvalid_0's log_loss: 0.585314\n",
      "[224]\tvalid_0's log_loss: 0.585207\n",
      "[225]\tvalid_0's log_loss: 0.584896\n",
      "[226]\tvalid_0's log_loss: 0.584995\n",
      "[227]\tvalid_0's log_loss: 0.585147\n",
      "[228]\tvalid_0's log_loss: 0.584969\n",
      "[229]\tvalid_0's log_loss: 0.585034\n",
      "[230]\tvalid_0's log_loss: 0.584975\n",
      "[231]\tvalid_0's log_loss: 0.585211\n",
      "[232]\tvalid_0's log_loss: 0.585589\n",
      "[233]\tvalid_0's log_loss: 0.58562\n",
      "[234]\tvalid_0's log_loss: 0.585751\n",
      "[235]\tvalid_0's log_loss: 0.585795\n",
      "[236]\tvalid_0's log_loss: 0.585776\n",
      "[237]\tvalid_0's log_loss: 0.585826\n",
      "[238]\tvalid_0's log_loss: 0.585811\n",
      "[239]\tvalid_0's log_loss: 0.585808\n",
      "[240]\tvalid_0's log_loss: 0.586481\n",
      "[241]\tvalid_0's log_loss: 0.58662\n",
      "[242]\tvalid_0's log_loss: 0.586653\n",
      "[243]\tvalid_0's log_loss: 0.586898\n",
      "[244]\tvalid_0's log_loss: 0.586936\n",
      "[245]\tvalid_0's log_loss: 0.58673\n",
      "[246]\tvalid_0's log_loss: 0.5866\n",
      "[247]\tvalid_0's log_loss: 0.586729\n",
      "[248]\tvalid_0's log_loss: 0.58681\n",
      "[249]\tvalid_0's log_loss: 0.58704\n",
      "[250]\tvalid_0's log_loss: 0.586854\n",
      "[251]\tvalid_0's log_loss: 0.587098\n",
      "[252]\tvalid_0's log_loss: 0.587246\n",
      "[253]\tvalid_0's log_loss: 0.586803\n",
      "[254]\tvalid_0's log_loss: 0.586966\n",
      "[255]\tvalid_0's log_loss: 0.586462\n",
      "[256]\tvalid_0's log_loss: 0.586323\n",
      "[257]\tvalid_0's log_loss: 0.586656\n",
      "[258]\tvalid_0's log_loss: 0.58609\n",
      "[259]\tvalid_0's log_loss: 0.586076\n",
      "[260]\tvalid_0's log_loss: 0.585811\n",
      "[261]\tvalid_0's log_loss: 0.585976\n",
      "[262]\tvalid_0's log_loss: 0.585875\n",
      "[263]\tvalid_0's log_loss: 0.586077\n",
      "[264]\tvalid_0's log_loss: 0.586\n",
      "[265]\tvalid_0's log_loss: 0.585876\n",
      "[266]\tvalid_0's log_loss: 0.586096\n",
      "[267]\tvalid_0's log_loss: 0.585995\n",
      "[268]\tvalid_0's log_loss: 0.586026\n",
      "[269]\tvalid_0's log_loss: 0.586078\n",
      "[270]\tvalid_0's log_loss: 0.58628\n",
      "[271]\tvalid_0's log_loss: 0.586457\n",
      "[272]\tvalid_0's log_loss: 0.58669\n",
      "[273]\tvalid_0's log_loss: 0.586614\n",
      "[274]\tvalid_0's log_loss: 0.586336\n",
      "[275]\tvalid_0's log_loss: 0.586162\n",
      "[276]\tvalid_0's log_loss: 0.586088\n",
      "[277]\tvalid_0's log_loss: 0.586084\n",
      "[278]\tvalid_0's log_loss: 0.585894\n",
      "[279]\tvalid_0's log_loss: 0.586003\n",
      "[280]\tvalid_0's log_loss: 0.586119\n",
      "[281]\tvalid_0's log_loss: 0.586441\n",
      "[282]\tvalid_0's log_loss: 0.58681\n",
      "[283]\tvalid_0's log_loss: 0.586679\n",
      "[284]\tvalid_0's log_loss: 0.586766\n",
      "[285]\tvalid_0's log_loss: 0.586828\n",
      "[286]\tvalid_0's log_loss: 0.586641\n",
      "[287]\tvalid_0's log_loss: 0.586558\n",
      "[288]\tvalid_0's log_loss: 0.586399\n",
      "[289]\tvalid_0's log_loss: 0.586493\n",
      "[290]\tvalid_0's log_loss: 0.58622\n",
      "[291]\tvalid_0's log_loss: 0.585937\n",
      "[292]\tvalid_0's log_loss: 0.585506\n",
      "[293]\tvalid_0's log_loss: 0.585576\n",
      "[294]\tvalid_0's log_loss: 0.585843\n",
      "[295]\tvalid_0's log_loss: 0.585651\n",
      "[296]\tvalid_0's log_loss: 0.585663\n",
      "[297]\tvalid_0's log_loss: 0.585821\n",
      "[298]\tvalid_0's log_loss: 0.585904\n",
      "[299]\tvalid_0's log_loss: 0.585951\n",
      "[300]\tvalid_0's log_loss: 0.586157\n",
      "[301]\tvalid_0's log_loss: 0.586294\n",
      "[302]\tvalid_0's log_loss: 0.586392\n",
      "[303]\tvalid_0's log_loss: 0.586215\n",
      "[304]\tvalid_0's log_loss: 0.586456\n",
      "[305]\tvalid_0's log_loss: 0.586625\n",
      "[306]\tvalid_0's log_loss: 0.586653\n",
      "[307]\tvalid_0's log_loss: 0.586444\n",
      "[308]\tvalid_0's log_loss: 0.586758\n",
      "[309]\tvalid_0's log_loss: 0.586759\n",
      "[310]\tvalid_0's log_loss: 0.586814\n",
      "[311]\tvalid_0's log_loss: 0.586871\n",
      "[312]\tvalid_0's log_loss: 0.587001\n",
      "[313]\tvalid_0's log_loss: 0.586952\n",
      "[314]\tvalid_0's log_loss: 0.58693\n",
      "[315]\tvalid_0's log_loss: 0.587058\n",
      "[316]\tvalid_0's log_loss: 0.587003\n",
      "[317]\tvalid_0's log_loss: 0.586899\n",
      "[318]\tvalid_0's log_loss: 0.586745\n",
      "[319]\tvalid_0's log_loss: 0.586727\n",
      "[320]\tvalid_0's log_loss: 0.587134\n",
      "[321]\tvalid_0's log_loss: 0.587255\n",
      "[322]\tvalid_0's log_loss: 0.587331\n",
      "[323]\tvalid_0's log_loss: 0.587495\n",
      "[324]\tvalid_0's log_loss: 0.587345\n",
      "[325]\tvalid_0's log_loss: 0.587534\n",
      "[326]\tvalid_0's log_loss: 0.587507\n",
      "[327]\tvalid_0's log_loss: 0.587854\n",
      "[328]\tvalid_0's log_loss: 0.58774\n",
      "[329]\tvalid_0's log_loss: 0.587917\n",
      "[330]\tvalid_0's log_loss: 0.587802\n",
      "[331]\tvalid_0's log_loss: 0.587693\n",
      "[332]\tvalid_0's log_loss: 0.587753\n",
      "[333]\tvalid_0's log_loss: 0.587699\n",
      "[334]\tvalid_0's log_loss: 0.58766\n",
      "[335]\tvalid_0's log_loss: 0.587948\n",
      "[336]\tvalid_0's log_loss: 0.587884\n",
      "[337]\tvalid_0's log_loss: 0.587912\n",
      "[338]\tvalid_0's log_loss: 0.587974\n",
      "[339]\tvalid_0's log_loss: 0.587906\n",
      "[340]\tvalid_0's log_loss: 0.587819\n",
      "[341]\tvalid_0's log_loss: 0.587663\n",
      "[342]\tvalid_0's log_loss: 0.587931\n",
      "[343]\tvalid_0's log_loss: 0.587954\n",
      "[344]\tvalid_0's log_loss: 0.58765\n",
      "[345]\tvalid_0's log_loss: 0.587619\n",
      "[346]\tvalid_0's log_loss: 0.587591\n",
      "[347]\tvalid_0's log_loss: 0.587532\n",
      "[348]\tvalid_0's log_loss: 0.587662\n",
      "[349]\tvalid_0's log_loss: 0.587951\n",
      "[350]\tvalid_0's log_loss: 0.587596\n",
      "[351]\tvalid_0's log_loss: 0.587349\n",
      "[352]\tvalid_0's log_loss: 0.587089\n",
      "[353]\tvalid_0's log_loss: 0.587328\n",
      "[354]\tvalid_0's log_loss: 0.587374\n",
      "[355]\tvalid_0's log_loss: 0.587382\n",
      "[356]\tvalid_0's log_loss: 0.587763\n",
      "[357]\tvalid_0's log_loss: 0.587427\n",
      "[358]\tvalid_0's log_loss: 0.587711\n",
      "[359]\tvalid_0's log_loss: 0.58768\n",
      "[360]\tvalid_0's log_loss: 0.587492\n",
      "[361]\tvalid_0's log_loss: 0.58756\n",
      "[362]\tvalid_0's log_loss: 0.587602\n",
      "[363]\tvalid_0's log_loss: 0.587372\n",
      "[364]\tvalid_0's log_loss: 0.587551\n",
      "[365]\tvalid_0's log_loss: 0.58756\n",
      "[366]\tvalid_0's log_loss: 0.587658\n",
      "[367]\tvalid_0's log_loss: 0.587968\n",
      "[368]\tvalid_0's log_loss: 0.587704\n",
      "[369]\tvalid_0's log_loss: 0.587672\n",
      "[370]\tvalid_0's log_loss: 0.58801\n",
      "[371]\tvalid_0's log_loss: 0.588143\n",
      "[372]\tvalid_0's log_loss: 0.587974\n",
      "[373]\tvalid_0's log_loss: 0.588126\n",
      "[374]\tvalid_0's log_loss: 0.588121\n",
      "[375]\tvalid_0's log_loss: 0.58833\n",
      "[376]\tvalid_0's log_loss: 0.588439\n",
      "[377]\tvalid_0's log_loss: 0.588439\n",
      "[378]\tvalid_0's log_loss: 0.588163\n",
      "[379]\tvalid_0's log_loss: 0.588136\n",
      "[380]\tvalid_0's log_loss: 0.588196\n",
      "[381]\tvalid_0's log_loss: 0.588188\n",
      "[382]\tvalid_0's log_loss: 0.588364\n",
      "[383]\tvalid_0's log_loss: 0.588452\n",
      "[384]\tvalid_0's log_loss: 0.588374\n",
      "[385]\tvalid_0's log_loss: 0.588189\n",
      "[386]\tvalid_0's log_loss: 0.588114\n",
      "[387]\tvalid_0's log_loss: 0.587864\n",
      "[388]\tvalid_0's log_loss: 0.58817\n",
      "[389]\tvalid_0's log_loss: 0.588299\n",
      "[390]\tvalid_0's log_loss: 0.588679\n",
      "[391]\tvalid_0's log_loss: 0.588772\n",
      "[392]\tvalid_0's log_loss: 0.588958\n",
      "[393]\tvalid_0's log_loss: 0.589001\n",
      "[394]\tvalid_0's log_loss: 0.589148\n",
      "[395]\tvalid_0's log_loss: 0.588898\n",
      "[396]\tvalid_0's log_loss: 0.589292\n",
      "[397]\tvalid_0's log_loss: 0.589269\n",
      "[398]\tvalid_0's log_loss: 0.589724\n",
      "[399]\tvalid_0's log_loss: 0.589816\n",
      "[400]\tvalid_0's log_loss: 0.589792\n",
      "[401]\tvalid_0's log_loss: 0.589693\n",
      "[402]\tvalid_0's log_loss: 0.589892\n",
      "[403]\tvalid_0's log_loss: 0.589798\n",
      "[404]\tvalid_0's log_loss: 0.589805\n",
      "[405]\tvalid_0's log_loss: 0.590023\n",
      "[406]\tvalid_0's log_loss: 0.59011\n",
      "[407]\tvalid_0's log_loss: 0.59021\n",
      "[408]\tvalid_0's log_loss: 0.590344\n",
      "[409]\tvalid_0's log_loss: 0.590414\n",
      "[410]\tvalid_0's log_loss: 0.590283\n",
      "[411]\tvalid_0's log_loss: 0.590171\n",
      "[412]\tvalid_0's log_loss: 0.590059\n",
      "[413]\tvalid_0's log_loss: 0.589906\n",
      "[414]\tvalid_0's log_loss: 0.589881\n",
      "[415]\tvalid_0's log_loss: 0.589988\n",
      "[416]\tvalid_0's log_loss: 0.590124\n",
      "[417]\tvalid_0's log_loss: 0.590182\n",
      "[418]\tvalid_0's log_loss: 0.590148\n",
      "[419]\tvalid_0's log_loss: 0.590236\n",
      "[420]\tvalid_0's log_loss: 0.59028\n",
      "[421]\tvalid_0's log_loss: 0.590071\n",
      "[422]\tvalid_0's log_loss: 0.589807\n",
      "[423]\tvalid_0's log_loss: 0.589612\n",
      "[424]\tvalid_0's log_loss: 0.589618\n",
      "[425]\tvalid_0's log_loss: 0.589897\n",
      "[426]\tvalid_0's log_loss: 0.589998\n",
      "[427]\tvalid_0's log_loss: 0.59004\n",
      "[428]\tvalid_0's log_loss: 0.590177\n",
      "[429]\tvalid_0's log_loss: 0.59014\n",
      "[430]\tvalid_0's log_loss: 0.590237\n",
      "[431]\tvalid_0's log_loss: 0.590121\n",
      "[432]\tvalid_0's log_loss: 0.590401\n",
      "[433]\tvalid_0's log_loss: 0.590578\n",
      "[434]\tvalid_0's log_loss: 0.59066\n",
      "[435]\tvalid_0's log_loss: 0.590658\n",
      "[436]\tvalid_0's log_loss: 0.591083\n",
      "[437]\tvalid_0's log_loss: 0.591147\n",
      "[438]\tvalid_0's log_loss: 0.591228\n",
      "[439]\tvalid_0's log_loss: 0.591241\n",
      "[440]\tvalid_0's log_loss: 0.591624\n",
      "[441]\tvalid_0's log_loss: 0.591657\n",
      "[442]\tvalid_0's log_loss: 0.59151\n",
      "[443]\tvalid_0's log_loss: 0.591609\n",
      "[444]\tvalid_0's log_loss: 0.591855\n",
      "[445]\tvalid_0's log_loss: 0.591835\n",
      "[446]\tvalid_0's log_loss: 0.591848\n",
      "[447]\tvalid_0's log_loss: 0.591861\n",
      "[448]\tvalid_0's log_loss: 0.591788\n",
      "Early stopping, best iteration is:\n",
      "[148]\tvalid_0's log_loss: 0.584084\n",
      "[1]\tvalid_0's log_loss: 0.559635\n",
      "Train until valid scores didn't improve in 300 rounds.\n",
      "[2]\tvalid_0's log_loss: 0.559704\n",
      "[3]\tvalid_0's log_loss: 0.559567\n",
      "[4]\tvalid_0's log_loss: 0.559919\n",
      "[5]\tvalid_0's log_loss: 0.559743\n",
      "[6]\tvalid_0's log_loss: 0.559575\n",
      "[7]\tvalid_0's log_loss: 0.559308\n",
      "[8]\tvalid_0's log_loss: 0.558937\n",
      "[9]\tvalid_0's log_loss: 0.558976\n",
      "[10]\tvalid_0's log_loss: 0.559141\n",
      "[11]\tvalid_0's log_loss: 0.558958\n",
      "[12]\tvalid_0's log_loss: 0.558959\n",
      "[13]\tvalid_0's log_loss: 0.559238\n",
      "[14]\tvalid_0's log_loss: 0.559115\n",
      "[15]\tvalid_0's log_loss: 0.559167\n",
      "[16]\tvalid_0's log_loss: 0.559571\n",
      "[17]\tvalid_0's log_loss: 0.560071\n",
      "[18]\tvalid_0's log_loss: 0.559688\n",
      "[19]\tvalid_0's log_loss: 0.559707\n",
      "[20]\tvalid_0's log_loss: 0.559019\n",
      "[21]\tvalid_0's log_loss: 0.558938\n",
      "[22]\tvalid_0's log_loss: 0.558864\n",
      "[23]\tvalid_0's log_loss: 0.55893\n",
      "[24]\tvalid_0's log_loss: 0.558418\n",
      "[25]\tvalid_0's log_loss: 0.558522\n",
      "[26]\tvalid_0's log_loss: 0.558368\n",
      "[27]\tvalid_0's log_loss: 0.558116\n",
      "[28]\tvalid_0's log_loss: 0.55843\n",
      "[29]\tvalid_0's log_loss: 0.558475\n",
      "[30]\tvalid_0's log_loss: 0.557721\n",
      "[31]\tvalid_0's log_loss: 0.557389\n",
      "[32]\tvalid_0's log_loss: 0.557317\n",
      "[33]\tvalid_0's log_loss: 0.556852\n",
      "[34]\tvalid_0's log_loss: 0.556756\n",
      "[35]\tvalid_0's log_loss: 0.557173\n",
      "[36]\tvalid_0's log_loss: 0.556966\n",
      "[37]\tvalid_0's log_loss: 0.556699\n",
      "[38]\tvalid_0's log_loss: 0.55649\n",
      "[39]\tvalid_0's log_loss: 0.556508\n",
      "[40]\tvalid_0's log_loss: 0.556441\n",
      "[41]\tvalid_0's log_loss: 0.556463\n",
      "[42]\tvalid_0's log_loss: 0.556397\n",
      "[43]\tvalid_0's log_loss: 0.556468\n",
      "[44]\tvalid_0's log_loss: 0.556693\n",
      "[45]\tvalid_0's log_loss: 0.55648\n",
      "[46]\tvalid_0's log_loss: 0.556456\n",
      "[47]\tvalid_0's log_loss: 0.556274\n",
      "[48]\tvalid_0's log_loss: 0.556353\n",
      "[49]\tvalid_0's log_loss: 0.556396\n",
      "[50]\tvalid_0's log_loss: 0.556848\n",
      "[51]\tvalid_0's log_loss: 0.557117\n",
      "[52]\tvalid_0's log_loss: 0.556931\n",
      "[53]\tvalid_0's log_loss: 0.557035\n",
      "[54]\tvalid_0's log_loss: 0.557114\n",
      "[55]\tvalid_0's log_loss: 0.557255\n",
      "[56]\tvalid_0's log_loss: 0.557419\n",
      "[57]\tvalid_0's log_loss: 0.557348\n",
      "[58]\tvalid_0's log_loss: 0.557585\n",
      "[59]\tvalid_0's log_loss: 0.557386\n",
      "[60]\tvalid_0's log_loss: 0.55745\n",
      "[61]\tvalid_0's log_loss: 0.556968\n",
      "[62]\tvalid_0's log_loss: 0.556784\n",
      "[63]\tvalid_0's log_loss: 0.556569\n",
      "[64]\tvalid_0's log_loss: 0.556574\n",
      "[65]\tvalid_0's log_loss: 0.556667\n",
      "[66]\tvalid_0's log_loss: 0.556672\n",
      "[67]\tvalid_0's log_loss: 0.556565\n",
      "[68]\tvalid_0's log_loss: 0.557021\n",
      "[69]\tvalid_0's log_loss: 0.556892\n",
      "[70]\tvalid_0's log_loss: 0.556843\n",
      "[71]\tvalid_0's log_loss: 0.556962\n",
      "[72]\tvalid_0's log_loss: 0.557319\n",
      "[73]\tvalid_0's log_loss: 0.557331\n",
      "[74]\tvalid_0's log_loss: 0.557555\n",
      "[75]\tvalid_0's log_loss: 0.557308\n",
      "[76]\tvalid_0's log_loss: 0.557316\n",
      "[77]\tvalid_0's log_loss: 0.557153\n",
      "[78]\tvalid_0's log_loss: 0.557106\n",
      "[79]\tvalid_0's log_loss: 0.55705\n",
      "[80]\tvalid_0's log_loss: 0.557572\n",
      "[81]\tvalid_0's log_loss: 0.557865\n",
      "[82]\tvalid_0's log_loss: 0.557723\n",
      "[83]\tvalid_0's log_loss: 0.557593\n",
      "[84]\tvalid_0's log_loss: 0.557544\n",
      "[85]\tvalid_0's log_loss: 0.557559\n",
      "[86]\tvalid_0's log_loss: 0.557673\n",
      "[87]\tvalid_0's log_loss: 0.557775\n",
      "[88]\tvalid_0's log_loss: 0.557597\n",
      "[89]\tvalid_0's log_loss: 0.557483\n",
      "[90]\tvalid_0's log_loss: 0.557379\n",
      "[91]\tvalid_0's log_loss: 0.557543\n",
      "[92]\tvalid_0's log_loss: 0.557435\n",
      "[93]\tvalid_0's log_loss: 0.557491\n",
      "[94]\tvalid_0's log_loss: 0.557974\n",
      "[95]\tvalid_0's log_loss: 0.558425\n",
      "[96]\tvalid_0's log_loss: 0.558719\n",
      "[97]\tvalid_0's log_loss: 0.558686\n",
      "[98]\tvalid_0's log_loss: 0.558287\n",
      "[99]\tvalid_0's log_loss: 0.558349\n",
      "[100]\tvalid_0's log_loss: 0.558005\n",
      "[101]\tvalid_0's log_loss: 0.558335\n",
      "[102]\tvalid_0's log_loss: 0.558142\n",
      "[103]\tvalid_0's log_loss: 0.558259\n",
      "[104]\tvalid_0's log_loss: 0.558383\n",
      "[105]\tvalid_0's log_loss: 0.558329\n",
      "[106]\tvalid_0's log_loss: 0.558698\n",
      "[107]\tvalid_0's log_loss: 0.558814\n",
      "[108]\tvalid_0's log_loss: 0.55859\n",
      "[109]\tvalid_0's log_loss: 0.558574\n",
      "[110]\tvalid_0's log_loss: 0.55858\n",
      "[111]\tvalid_0's log_loss: 0.55849\n",
      "[112]\tvalid_0's log_loss: 0.558598\n",
      "[113]\tvalid_0's log_loss: 0.558424\n",
      "[114]\tvalid_0's log_loss: 0.558325\n",
      "[115]\tvalid_0's log_loss: 0.558066\n",
      "[116]\tvalid_0's log_loss: 0.558266\n",
      "[117]\tvalid_0's log_loss: 0.558446\n",
      "[118]\tvalid_0's log_loss: 0.558566\n",
      "[119]\tvalid_0's log_loss: 0.558824\n",
      "[120]\tvalid_0's log_loss: 0.55876\n",
      "[121]\tvalid_0's log_loss: 0.558905\n",
      "[122]\tvalid_0's log_loss: 0.558401\n",
      "[123]\tvalid_0's log_loss: 0.559\n",
      "[124]\tvalid_0's log_loss: 0.559125\n",
      "[125]\tvalid_0's log_loss: 0.558728\n",
      "[126]\tvalid_0's log_loss: 0.558634\n",
      "[127]\tvalid_0's log_loss: 0.558801\n",
      "[128]\tvalid_0's log_loss: 0.55899\n",
      "[129]\tvalid_0's log_loss: 0.559102\n",
      "[130]\tvalid_0's log_loss: 0.559209\n",
      "[131]\tvalid_0's log_loss: 0.559089\n",
      "[132]\tvalid_0's log_loss: 0.559055\n",
      "[133]\tvalid_0's log_loss: 0.559327\n",
      "[134]\tvalid_0's log_loss: 0.559428\n",
      "[135]\tvalid_0's log_loss: 0.559699\n",
      "[136]\tvalid_0's log_loss: 0.559815\n",
      "[137]\tvalid_0's log_loss: 0.559733\n",
      "[138]\tvalid_0's log_loss: 0.55989\n",
      "[139]\tvalid_0's log_loss: 0.559705\n",
      "[140]\tvalid_0's log_loss: 0.559779\n",
      "[141]\tvalid_0's log_loss: 0.560093\n",
      "[142]\tvalid_0's log_loss: 0.559964\n",
      "[143]\tvalid_0's log_loss: 0.559725\n",
      "[144]\tvalid_0's log_loss: 0.560117\n",
      "[145]\tvalid_0's log_loss: 0.56005\n",
      "[146]\tvalid_0's log_loss: 0.559948\n",
      "[147]\tvalid_0's log_loss: 0.560301\n",
      "[148]\tvalid_0's log_loss: 0.559945\n",
      "[149]\tvalid_0's log_loss: 0.560189\n",
      "[150]\tvalid_0's log_loss: 0.560109\n",
      "[151]\tvalid_0's log_loss: 0.560124\n",
      "[152]\tvalid_0's log_loss: 0.560042\n",
      "[153]\tvalid_0's log_loss: 0.559965\n",
      "[154]\tvalid_0's log_loss: 0.560008\n",
      "[155]\tvalid_0's log_loss: 0.559973\n",
      "[156]\tvalid_0's log_loss: 0.560216\n",
      "[157]\tvalid_0's log_loss: 0.560457\n",
      "[158]\tvalid_0's log_loss: 0.560253\n",
      "[159]\tvalid_0's log_loss: 0.560208\n",
      "[160]\tvalid_0's log_loss: 0.560354\n",
      "[161]\tvalid_0's log_loss: 0.560506\n",
      "[162]\tvalid_0's log_loss: 0.560691\n",
      "[163]\tvalid_0's log_loss: 0.560452\n",
      "[164]\tvalid_0's log_loss: 0.560617\n",
      "[165]\tvalid_0's log_loss: 0.560695\n",
      "[166]\tvalid_0's log_loss: 0.56097\n",
      "[167]\tvalid_0's log_loss: 0.560862\n",
      "[168]\tvalid_0's log_loss: 0.560882\n",
      "[169]\tvalid_0's log_loss: 0.561115\n",
      "[170]\tvalid_0's log_loss: 0.561526\n",
      "[171]\tvalid_0's log_loss: 0.561556\n",
      "[172]\tvalid_0's log_loss: 0.561696\n",
      "[173]\tvalid_0's log_loss: 0.561541\n",
      "[174]\tvalid_0's log_loss: 0.56188\n",
      "[175]\tvalid_0's log_loss: 0.561809\n",
      "[176]\tvalid_0's log_loss: 0.56185\n",
      "[177]\tvalid_0's log_loss: 0.561989\n",
      "[178]\tvalid_0's log_loss: 0.561842\n",
      "[179]\tvalid_0's log_loss: 0.561734\n",
      "[180]\tvalid_0's log_loss: 0.56165\n",
      "[181]\tvalid_0's log_loss: 0.561796\n",
      "[182]\tvalid_0's log_loss: 0.561725\n",
      "[183]\tvalid_0's log_loss: 0.561782\n",
      "[184]\tvalid_0's log_loss: 0.561694\n",
      "[185]\tvalid_0's log_loss: 0.561196\n",
      "[186]\tvalid_0's log_loss: 0.561335\n",
      "[187]\tvalid_0's log_loss: 0.56103\n",
      "[188]\tvalid_0's log_loss: 0.561159\n",
      "[189]\tvalid_0's log_loss: 0.561075\n",
      "[190]\tvalid_0's log_loss: 0.56127\n",
      "[191]\tvalid_0's log_loss: 0.561386\n",
      "[192]\tvalid_0's log_loss: 0.561565\n",
      "[193]\tvalid_0's log_loss: 0.561685\n",
      "[194]\tvalid_0's log_loss: 0.561964\n",
      "[195]\tvalid_0's log_loss: 0.561965\n",
      "[196]\tvalid_0's log_loss: 0.561838\n",
      "[197]\tvalid_0's log_loss: 0.561765\n",
      "[198]\tvalid_0's log_loss: 0.56172\n",
      "[199]\tvalid_0's log_loss: 0.561678\n",
      "[200]\tvalid_0's log_loss: 0.561468\n",
      "[201]\tvalid_0's log_loss: 0.56139\n",
      "[202]\tvalid_0's log_loss: 0.561517\n",
      "[203]\tvalid_0's log_loss: 0.561611\n",
      "[204]\tvalid_0's log_loss: 0.561629\n",
      "[205]\tvalid_0's log_loss: 0.561885\n",
      "[206]\tvalid_0's log_loss: 0.562036\n",
      "[207]\tvalid_0's log_loss: 0.562303\n",
      "[208]\tvalid_0's log_loss: 0.562599\n",
      "[209]\tvalid_0's log_loss: 0.562538\n",
      "[210]\tvalid_0's log_loss: 0.562684\n",
      "[211]\tvalid_0's log_loss: 0.562434\n",
      "[212]\tvalid_0's log_loss: 0.562615\n",
      "[213]\tvalid_0's log_loss: 0.56277\n",
      "[214]\tvalid_0's log_loss: 0.562958\n",
      "[215]\tvalid_0's log_loss: 0.563167\n",
      "[216]\tvalid_0's log_loss: 0.563341\n",
      "[217]\tvalid_0's log_loss: 0.563489\n",
      "[218]\tvalid_0's log_loss: 0.563587\n",
      "[219]\tvalid_0's log_loss: 0.563696\n",
      "[220]\tvalid_0's log_loss: 0.56359\n",
      "[221]\tvalid_0's log_loss: 0.563772\n",
      "[222]\tvalid_0's log_loss: 0.56376\n",
      "[223]\tvalid_0's log_loss: 0.563848\n",
      "[224]\tvalid_0's log_loss: 0.564171\n",
      "[225]\tvalid_0's log_loss: 0.564125\n",
      "[226]\tvalid_0's log_loss: 0.564263\n",
      "[227]\tvalid_0's log_loss: 0.564137\n",
      "[228]\tvalid_0's log_loss: 0.564439\n",
      "[229]\tvalid_0's log_loss: 0.564461\n",
      "[230]\tvalid_0's log_loss: 0.564786\n",
      "[231]\tvalid_0's log_loss: 0.564743\n",
      "[232]\tvalid_0's log_loss: 0.564679\n",
      "[233]\tvalid_0's log_loss: 0.564944\n",
      "[234]\tvalid_0's log_loss: 0.565346\n",
      "[235]\tvalid_0's log_loss: 0.565512\n",
      "[236]\tvalid_0's log_loss: 0.565675\n",
      "[237]\tvalid_0's log_loss: 0.565725\n",
      "[238]\tvalid_0's log_loss: 0.565677\n",
      "[239]\tvalid_0's log_loss: 0.56569\n",
      "[240]\tvalid_0's log_loss: 0.565971\n",
      "[241]\tvalid_0's log_loss: 0.566013\n",
      "[242]\tvalid_0's log_loss: 0.566191\n",
      "[243]\tvalid_0's log_loss: 0.566108\n",
      "[244]\tvalid_0's log_loss: 0.566266\n",
      "[245]\tvalid_0's log_loss: 0.566447\n",
      "[246]\tvalid_0's log_loss: 0.566488\n",
      "[247]\tvalid_0's log_loss: 0.566726\n",
      "[248]\tvalid_0's log_loss: 0.56683\n",
      "[249]\tvalid_0's log_loss: 0.567038\n",
      "[250]\tvalid_0's log_loss: 0.567226\n",
      "[251]\tvalid_0's log_loss: 0.567802\n",
      "[252]\tvalid_0's log_loss: 0.567637\n",
      "[253]\tvalid_0's log_loss: 0.56767\n",
      "[254]\tvalid_0's log_loss: 0.56771\n",
      "[255]\tvalid_0's log_loss: 0.568002\n",
      "[256]\tvalid_0's log_loss: 0.567701\n",
      "[257]\tvalid_0's log_loss: 0.567843\n",
      "[258]\tvalid_0's log_loss: 0.567785\n",
      "[259]\tvalid_0's log_loss: 0.567575\n",
      "[260]\tvalid_0's log_loss: 0.567272\n",
      "[261]\tvalid_0's log_loss: 0.567183\n",
      "[262]\tvalid_0's log_loss: 0.567113\n",
      "[263]\tvalid_0's log_loss: 0.566696\n",
      "[264]\tvalid_0's log_loss: 0.56687\n",
      "[265]\tvalid_0's log_loss: 0.566923\n",
      "[266]\tvalid_0's log_loss: 0.567095\n",
      "[267]\tvalid_0's log_loss: 0.56728\n",
      "[268]\tvalid_0's log_loss: 0.567067\n",
      "[269]\tvalid_0's log_loss: 0.56716\n",
      "[270]\tvalid_0's log_loss: 0.567189\n",
      "[271]\tvalid_0's log_loss: 0.567483\n",
      "[272]\tvalid_0's log_loss: 0.567517\n",
      "[273]\tvalid_0's log_loss: 0.567244\n",
      "[274]\tvalid_0's log_loss: 0.567435\n",
      "[275]\tvalid_0's log_loss: 0.567392\n",
      "[276]\tvalid_0's log_loss: 0.567538\n",
      "[277]\tvalid_0's log_loss: 0.567318\n",
      "[278]\tvalid_0's log_loss: 0.567344\n",
      "[279]\tvalid_0's log_loss: 0.56718\n",
      "[280]\tvalid_0's log_loss: 0.567249\n",
      "[281]\tvalid_0's log_loss: 0.567153\n",
      "[282]\tvalid_0's log_loss: 0.567292\n",
      "[283]\tvalid_0's log_loss: 0.567286\n",
      "[284]\tvalid_0's log_loss: 0.567575\n",
      "[285]\tvalid_0's log_loss: 0.567505\n",
      "[286]\tvalid_0's log_loss: 0.567569\n",
      "[287]\tvalid_0's log_loss: 0.567311\n",
      "[288]\tvalid_0's log_loss: 0.567201\n",
      "[289]\tvalid_0's log_loss: 0.56722\n",
      "[290]\tvalid_0's log_loss: 0.567217\n",
      "[291]\tvalid_0's log_loss: 0.567125\n",
      "[292]\tvalid_0's log_loss: 0.56726\n",
      "[293]\tvalid_0's log_loss: 0.567217\n",
      "[294]\tvalid_0's log_loss: 0.567414\n",
      "[295]\tvalid_0's log_loss: 0.567308\n",
      "[296]\tvalid_0's log_loss: 0.566938\n",
      "[297]\tvalid_0's log_loss: 0.566702\n",
      "[298]\tvalid_0's log_loss: 0.566551\n",
      "[299]\tvalid_0's log_loss: 0.566669\n",
      "[300]\tvalid_0's log_loss: 0.566789\n",
      "[301]\tvalid_0's log_loss: 0.566798\n",
      "[302]\tvalid_0's log_loss: 0.566737\n",
      "[303]\tvalid_0's log_loss: 0.566602\n",
      "[304]\tvalid_0's log_loss: 0.566456\n",
      "[305]\tvalid_0's log_loss: 0.566453\n",
      "[306]\tvalid_0's log_loss: 0.566521\n",
      "[307]\tvalid_0's log_loss: 0.566924\n",
      "[308]\tvalid_0's log_loss: 0.566743\n",
      "[309]\tvalid_0's log_loss: 0.567026\n",
      "[310]\tvalid_0's log_loss: 0.567071\n",
      "[311]\tvalid_0's log_loss: 0.567208\n",
      "[312]\tvalid_0's log_loss: 0.567136\n",
      "[313]\tvalid_0's log_loss: 0.567409\n",
      "[314]\tvalid_0's log_loss: 0.567671\n",
      "[315]\tvalid_0's log_loss: 0.567511\n",
      "[316]\tvalid_0's log_loss: 0.567636\n",
      "[317]\tvalid_0's log_loss: 0.567538\n",
      "[318]\tvalid_0's log_loss: 0.567707\n",
      "[319]\tvalid_0's log_loss: 0.567763\n",
      "[320]\tvalid_0's log_loss: 0.567935\n",
      "[321]\tvalid_0's log_loss: 0.567739\n",
      "[322]\tvalid_0's log_loss: 0.567805\n",
      "[323]\tvalid_0's log_loss: 0.567805\n",
      "[324]\tvalid_0's log_loss: 0.567751\n",
      "[325]\tvalid_0's log_loss: 0.567703\n",
      "[326]\tvalid_0's log_loss: 0.567993\n",
      "[327]\tvalid_0's log_loss: 0.56809\n",
      "[328]\tvalid_0's log_loss: 0.568066\n",
      "[329]\tvalid_0's log_loss: 0.567977\n",
      "[330]\tvalid_0's log_loss: 0.567906\n",
      "[331]\tvalid_0's log_loss: 0.567963\n",
      "[332]\tvalid_0's log_loss: 0.568008\n",
      "[333]\tvalid_0's log_loss: 0.568138\n",
      "[334]\tvalid_0's log_loss: 0.568141\n",
      "[335]\tvalid_0's log_loss: 0.568199\n",
      "[336]\tvalid_0's log_loss: 0.56798\n",
      "[337]\tvalid_0's log_loss: 0.567971\n",
      "[338]\tvalid_0's log_loss: 0.567621\n",
      "[339]\tvalid_0's log_loss: 0.56787\n",
      "[340]\tvalid_0's log_loss: 0.567794\n",
      "[341]\tvalid_0's log_loss: 0.567933\n",
      "[342]\tvalid_0's log_loss: 0.567765\n",
      "[343]\tvalid_0's log_loss: 0.567509\n",
      "[344]\tvalid_0's log_loss: 0.567899\n",
      "[345]\tvalid_0's log_loss: 0.567704\n",
      "[346]\tvalid_0's log_loss: 0.567668\n",
      "[347]\tvalid_0's log_loss: 0.56777\n",
      "Early stopping, best iteration is:\n",
      "[47]\tvalid_0's log_loss: 0.556274\n",
      "Training took 1435.859 sec.\n",
      "\n",
      "CPU times: user 18min 46s, sys: 5min 9s, total: 23min 55s\n",
      "Wall time: 2min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "clfs = compute_training(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Compute prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data of size (198, 2)\n",
      "Size of the penultimate layer (feature size): 2048\n",
      "Prediction took 67.833 sec.\n",
      "\n",
      "Results:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.27629386,  0.28223007,  0.27492749,  0.26843234,  0.33943704,\n",
       "        0.21081916,  0.20972817,  0.28702512,  0.24069078,  0.24932951,\n",
       "        0.18534621,  0.39019962,  0.22835822,  0.27146071,  0.19368372,\n",
       "        0.23800451,  0.22047086,  0.24788003,  0.2074103 ,  0.23167995,\n",
       "        0.52571891,  0.20634289,  0.28407315,  0.25869903,  0.21898748,\n",
       "        0.21651463,  0.20168881,  0.33051609,  0.3010676 ,  0.22267287,\n",
       "        0.21063386,  0.28224094,  0.36387866,  0.24424355,  0.27847576,\n",
       "        0.30605908,  0.31323599,  0.25855997,  0.24779951,  0.28504892,\n",
       "        0.2083872 ,  0.28949414,  0.21987275,  0.24687181,  0.25123812,\n",
       "        0.20563207,  0.2400069 ,  0.183639  ,  0.16695845,  0.2513766 ,\n",
       "        0.30234726,  0.22937858,  0.22933536,  0.24191114,  0.26528751,\n",
       "        0.22350396,  0.25859546,  0.35234748,  0.23233991,  0.22621058,\n",
       "        0.26095069,  0.23908669,  0.20278237,  0.21350358,  0.2969056 ,\n",
       "        0.43601893,  0.26746151,  0.19686179,  0.21629176,  0.22500693,\n",
       "        0.20892113,  0.25947121,  0.2799378 ,  0.23041469,  0.30279558,\n",
       "        0.19370794,  0.30534091,  0.257155  ,  0.23235112,  0.28348901,\n",
       "        0.22407198,  0.17500588,  0.4919361 ,  0.2306238 ,  0.19451813,\n",
       "        0.27263774,  0.27257424,  0.26110489,  0.26198984,  0.3021185 ,\n",
       "        0.30098056,  0.21056649,  0.2577423 ,  0.23421242,  0.25428234,\n",
       "        0.23763539,  0.17005178,  0.25207774,  0.32259793,  0.24486662,\n",
       "        0.24346289,  0.19738055,  0.18550539,  0.25852116,  0.29091648,\n",
       "        0.24982209,  0.17855858,  0.34297688,  0.2311484 ,  0.18503221,\n",
       "        0.23916923,  0.22370647,  0.20666918,  0.24576325,  0.21749971,\n",
       "        0.30644281,  0.29733306,  0.27504922,  0.227085  ,  0.25450273,\n",
       "        0.32672457,  0.20920604,  0.464257  ,  0.19079983,  0.3026824 ,\n",
       "        0.19091313,  0.41297505,  0.19144449,  0.37591891,  0.35712449,\n",
       "        0.24227449,  0.3607939 ,  0.23733843,  0.31322722,  0.20879155,\n",
       "        0.25435215,  0.21208738,  0.22770471,  0.21725376,  0.31806988,\n",
       "        0.33976478,  0.23078326,  0.2146858 ,  0.29696909,  0.25365488,\n",
       "        0.20231146,  0.23973894,  0.19499373,  0.22389511,  0.22148198,\n",
       "        0.30694292,  0.35089768,  0.26485589,  0.21429486,  0.1919641 ,\n",
       "        0.17109631,  0.2385956 ,  0.30990497,  0.23176318,  0.21909604,\n",
       "        0.24016844,  0.26961385,  0.17997949,  0.2448945 ,  0.24500207,\n",
       "        0.28331674,  0.27267486,  0.34756266,  0.21506701,  0.2078998 ,\n",
       "        0.24976577,  0.32551717,  0.23550295,  0.23415658,  0.25949504,\n",
       "        0.26828006,  0.21415364,  0.26321126,  0.49353278,  0.29971056,\n",
       "        0.21735583,  0.31040415,  0.23654197,  0.2199393 ,  0.23762151,\n",
       "        0.28540699,  0.20620325,  0.34618274,  0.31642531,  0.22507115,\n",
       "        0.30743982,  0.20506493,  0.29783441,  0.32510502,  0.28673211,\n",
       "        0.20218183,  0.21025245,  0.48691727])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = compute_prediction(clfs)\n",
    "print(\"Results:\")\n",
    "pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Save results to csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved in submit0042.csv\n"
     ]
    }
   ],
   "source": [
    "save_results(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Alternative routes and improvements\n",
    "This notebook is just an example of what you can do with CNTK and LightGBM. Here we present some alternative routes that you can implement based on this script:\n",
    "\n",
    "- Use other pretrained CNNs: As previously mentioned you can try other networks like [AlexNet](https://www.cntk.ai/Models/AlexNet/AlexNet.model), [AlexNet with Batch Normalization](https://www.cntk.ai/Models/AlexNet/AlexNetBS.model) and [ResNet with 18 layers](https://www.cntk.ai/Models/ResNet/ResNet_18.model). \n",
    "- Use transfer learning using a closer domain: The CNN we use is trained in ImageNet, which are natural images. There is another dataset called [LUNA](https://luna16.grand-challenge.org/) that you can use to train a network and then perform transfer learning. A trick that might speed up the convergence is to initialize the weights with a pretrained model (like we are doing in the notebook). \n",
    "- Perform image augmentation: You can try to increment the training set by performing transformations in the images. For that you can apply different filters or rotate them.\n",
    "- Use a customize network with 3D images: CNTK allows [3D convolutions](https://github.com/Microsoft/CNTK/wiki/Convolution). On GPU, 1D, 2D and 3D convolutions will use cuDNN (fast), all other convolutions will use reference engine (slow). You can create a CNN that accepts 3D images. \n",
    "- Try tunning the parameters of the boosted tree: You can try to tune the parameters of the tree. For that LightGBM implements the sklearn function `GridSearchCV`. Here you have an [example](https://github.com/Microsoft/LightGBM/blob/b0f7aa508a373b16adbda6cbe66b188a014964d8/examples/python-guide/sklearn_example.py).\n",
    "- Try some feature engineering: Before training the tree, the features are transformed. You can try to feed the tree without this operation or try a different one.\n",
    "\n",
    "After the competition was finished, some of the top performers published their solutions.\n",
    "\n",
    "- **Second place solution**: Julian de Wit was able to get to the second place in the Kaggle competition. Here you can find his [blog](http://juliandewit.github.io/kaggle-ndsb2017/) and here his [code](https://github.com/juliandewit/kaggle_ndsb2017). The solution is based on nodule detectors with a 3D convolutional neural network architecture.\n",
    "- **Ninth place solution**: A team composed by Andreas Verleysen, Elias Vansteenkiste, FrÃ©deric Godin, Ira Korshunova, Jonas Degrave, Lionel Pigou and Matthias Freiberger, all PhD students and postdocs at Ghent University, were able to reach the ninth position. They published a [blog](https://eliasvansteenkiste.github.io/machine%20learning/lung-cancer-pred/) and the [code](https://github.com/EliasVansteenkiste/dsb3). The pipeline has a 3D CNN for nodule segmentation, one CNN for false positive reduction, another CNN for identifying if the nodule is malignant or not, then transfer learning and finally ensembling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Acknowledgements\n",
    "\n",
    "This notebook is based on this [script published in kaggle](https://www.kaggle.com/hoaphumanoid/data-science-bowl-2017/cntk-and-lightgbm-quick-start). It was also based in this [other script](https://www.kaggle.com/drn01z3/data-science-bowl-2017/mxnet-xgboost-baseline-lb-0-57). "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
