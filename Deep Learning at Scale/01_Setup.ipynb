{"nbformat_minor": 2, "cells": [{"source": "# I - Setup\nThis notebook will set everything up for the tutorial. This notebook assumes that nothing has been set up previously and will create everything from scratch. The necessary steps are broken up into the following sections:\n* [Install tools and dependencies](#section1)\n* [Azure account login](#section2)\n* [Setup](#section3)\n* [Create Azure resources](#section4)\n* [Batch Shipyard Configuration](#section5)\n* [Create Azure Batch Pool](#section6)\n* [View Resources on Azure Portal or Batch Labs](#section7)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Install tools and dependencies\nInstall Batch Shipyard and Azure CLI 2.0", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!git clone https://github.com/Azure/batch-shipyard.git", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "Normally you would use `install.sh` or `install.cmd` helper scripts to install, but due to the Notebook environment, we will instead install with the `requirements.txt` file directly.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!pip install -r batch-shipyard/requirements.txt", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We will pre-download the Nvidia dependencies now to avoid the interactive license agreement prompt. If running outside of the Notebook environment then this step is not necessary.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!wget -O $HOME/batch-shipyard/resources/nvidia-docker.deb https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb\n!wget -O $HOME/batch-shipyard/resources/nvidia-driver.run http://us.download.nvidia.com/XFree86/Linux-x86_64/375.51/NVIDIA-Linux-x86_64-375.51.run", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Azure CLI 2.0 will also be installed to help us in provisioning Azure Batch and Storage accounts.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "!pip install -I azure-cli", "outputs": [], "metadata": {"collapsed": false}}, {"source": "We'll create an alias for invoking Batch Shipyard which points to a `config` directory which will hold our json config files (this directory will be created at a later step).", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section2'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Azure account login\nThe command below will initiate a login to your Azure account. It will pop up with an url to go to where you will enter a one off code and log into your Azure account using your browser if you do not specify a username and password or your organization requires multi-factor authentication.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "# login using browser device code\n!az login -o table\n\n# login using username and password (this may not work if your account requires multi-factor auth, use the above method instead)\n#USERNAME = \"account@domain.com\"    # replace with your username\n#PASSWORD = \"supersecretpassword\"  # replace with your password\n#!az login -u $USERNAME -p $PASSWORD -o table", "outputs": [], "metadata": {"collapsed": false}}, {"source": "If you have multiple subscriptions you can select the one you need with the command below.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "selected_subscription = '\"MY SUBSCRIPTION NAME\"' # Replace with the name of your subscription\n!az account set --subscription $selected_subscription", "outputs": [], "metadata": {"collapsed": true}}, {"source": "**Note:** If you cannot login with the Azure CLI, you can create Azure Batch and Storage accounts on the [Azure Portal](https://portal.azure.com).\n- [Instructions for creating an Azure Batch Account](https://docs.microsoft.com/en-us/azure/batch/batch-account-create-portal#batch-service-mode)\n- [Instructions for creating an Azure Storage Account](https://docs.microsoft.com/en-us/azure/storage/storage-create-storage-account#create-a-storage-account) (You can create an \"Auto Storage\" account at the same time as your Batch Account on the portal instead)\n\nPlease pay attention to special instructions regarding Azure Portal created accounts below.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup\nBelow are the various name definitions for the resources needed to run batch jobs.\n\n**Note:** If you manually created your accounts on the Azure Portal, you will need to modify `GROUP_NAME`, `BATCH_ACCOUNT_NAME` and `STORAGE_ACCOUNT_NAME` accordingly.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import json\nimport os\nimport uuid\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\nLOCATION = 'southcentralus' # We are setting everything up in South Central US\n                            # Be aware that you need to set things up in a region that has GPU VMs (N-Series)\n\nIMAGE_NAME = \"masalvar/cntkcifar\" # Custom CNTK image\n\n# Please do not modify below unless you created your accounts on the Azure Portal\n\nshort_uuid = str(uuid.uuid4())[:8]\n\nGROUP_NAME = \"batchcntk{uuid}rg\".format(uuid=short_uuid)\nBATCH_ACCOUNT_NAME = \"batchcntk{uuid}ba\".format(uuid=short_uuid)\nSTORAGE_ACCOUNT_NAME = \"batchcntk{uuid}str\".format(uuid=short_uuid)\nSTORAGE_ALIAS = \"mystorageaccount\"\nSTORAGE_ENDPOINT = \"core.windows.net\"", "outputs": [], "metadata": {"collapsed": true}}, {"source": "##### This cell is for recovery purposes only. There is no need to run this for normal executions:\n\nGROUP_NAME = \"\"\nBATCH_ACCOUNT_NAME = \"\"\nSTORAGE_ACCOUNT_NAME = \"\"\nbatch_account_key = \"\"\nbatch_service_url = \"\"\nstorage_account_key = \"\"", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section4'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Create Azure resources", "cell_type": "markdown", "metadata": {}}, {"source": "### Create Resource Group\nAzure encourages the use of resource groups to organise all the Azure components you deploy. That way it is easier to find them but also we can deleted a number of resources simply by deleting the Resource Group.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%%time\n!az group create -n $GROUP_NAME -l $LOCATION -o table", "outputs": [], "metadata": {"collapsed": false}}, {"source": "### Create Batch and Storage accounts\nHere we simply crate the Batch and Storage accounts. Once we have created the accounts we can the use the Azure CLI to query them and obtain the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for our Batch Shipyard configuration files later.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "json_data = !az storage account create -l $LOCATION -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME --sku Standard_LRS\nprint('Storage account provisioning state: {}'.format(json.loads(''.join(json_data))['provisioningState']))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "json_data = !az batch account create -l $LOCATION -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME --storage-account $STORAGE_ACCOUNT_NAME\nprint('Batch account provisioning state: {}'.format(json.loads(''.join(json_data))['provisioningState']))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Next we retrieve the **batch_account_key**, **batch_service_url** and **storage_account_key** which we will need for the Batch Shipyard configuration files further down.\n\n**Note:** If you created your Batch and Storage accounts in the Azure Portal, you will need to retrieve your keys in the Portal. Then set `batch_account_key`, `batch_service_url`, and `storage_account_key` to their appropriate values instead of the Azure CLI callouts.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "json_data = !az batch account keys list -n $BATCH_ACCOUNT_NAME -g $GROUP_NAME\nbatch_account_key = json.loads(''.join(json_data))['primary']", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "json_data = !az batch account list -g $GROUP_NAME\nbatch_service_url = 'https://'+json.loads(''.join(json_data))[0]['accountEndpoint']", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "json_data = !az storage account keys list -n $STORAGE_ACCOUNT_NAME -g $GROUP_NAME\nstorage_account_key = json.loads(''.join(json_data))[0]['value']", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Save credentials required for other notebooks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "account_information = {\n    \"storage_account_key\": storage_account_key,\n    \"storage_account_name\": STORAGE_ACCOUNT_NAME,\n    \"resource_group\": GROUP_NAME,\n    \"location\": LOCATION,\n}\nwrite_json_to_file(account_information, 'account_information.json')", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section5'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Batch Shiyard configuration\nIn order to execute a job on Batch Shipyard you need a minimum of four configuration files. We will set three of them here and leave the job one for later.\n* [credentials](#credentials)\n* [configuration](#configuration)\n* [pool](#pool)\n", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='credentials'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "### Credentials\nHere we define all the credentials necessary for Batch Shipyard to connect to Azure for resource provisioning and executing our jobs.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "credentials = {\n    \"credentials\": {\n        \"batch\": {\n            \"account_key\": batch_account_key,\n            \"account_service_url\": batch_service_url\n        },\n        \"storage\": {\n            STORAGE_ALIAS : {\n                    \"account\": STORAGE_ACCOUNT_NAME,\n                    \"account_key\": storage_account_key,\n                    \"endpoint\": STORAGE_ENDPOINT\n            }\n        }\n    }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='configuration'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "### Configuration\nThe config mainly contains the configuration for Batch Shipyard. Here we simply define the storage alias that Batch Shipyard should use as well as the Docker image to use.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "config = {\n    \"batch_shipyard\": {\n        \"storage_account_settings\": STORAGE_ALIAS\n    },\n    \"global_resources\": {\n        \"docker_images\": [\n            IMAGE_NAME\n        ]\n    }\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='pool'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "### Pool\nThis is where we define the properties of the compute pool we wish to create. The configuration below creates a pool that is made up of a three NC6 VM running Ubuntu. If you wish to run a job that uses GPUs then you need to use a VM from the NC series.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "POOL_ID = 'gpupool'\n\npool = {\n    \"pool_specification\": {\n        \"id\": POOL_ID,\n        \"vm_size\": \"STANDARD_NC6\",\n        \"vm_count\": 3,\n        \"publisher\": \"Canonical\",\n        \"offer\": \"UbuntuServer\",\n        \"sku\": \"16.04-LTS\",\n        \"ssh\": {\n            \"username\": \"docker\"\n        },\n        \"reboot_on_start_task_failed\": False,\n        \"block_until_all_global_resources_loaded\": True,\n    }\n}", "outputs": [], "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "!mkdir -p config # Create config file directory where we will store all our Batch Shipyard configuration files", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(credentials, os.path.join('config', 'credentials.json'))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(config, os.path.join('config', 'config.json'))", "outputs": [], "metadata": {"collapsed": false}}, {"execution_count": null, "cell_type": "code", "source": "write_json_to_file(pool, os.path.join('config', 'pool.json'))", "outputs": [], "metadata": {"collapsed": true}}, {"source": "<a id='section6'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Create Azure Batch Pool\nBefore we do anything we need to create the pool for Batch Shipyard jobs to run on. This can take a little bit of time so be patient.\n\n**Note:** As soon as one node enters `ComputeNodeState.idle` you can proceed you can proceed to the next notebook.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool add", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Once the pool is created we can confirm everything by running the command below.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Please run the following cell and copy/paste the information output from the cell into a scratch space (e.g., Notepad), just in case for recovery purposes:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "print('GROUP_NAME = \"{}\"'.format(GROUP_NAME))\nprint('BATCH_ACCOUNT_NAME = \"{}\"'.format(BATCH_ACCOUNT_NAME))\nprint('STORAGE_ACCOUNT_NAME = \"{}\"'.format(STORAGE_ACCOUNT_NAME))\nprint('batch_account_key = \"{}\"'.format(batch_account_key))\nprint('batch_service_url = \"{}\"'.format(batch_service_url))\nprint('storage_account_key = \"{}\"'.format(storage_account_key))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section7'><\/a>\n## View Resources on Azure Portal or Batch Labs", "cell_type": "markdown", "metadata": {}}, {"source": "If you want a graphical interface for your Azure Batch resources, you can view them on the [Azure Portal](https://portal.azure.com) or on [Batch Labs](https://github.com/Azure/BatchLabs).", "cell_type": "markdown", "metadata": {}}, {"source": "[Next notebook: Single GPU Training](02_Single_GPU_Training.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}