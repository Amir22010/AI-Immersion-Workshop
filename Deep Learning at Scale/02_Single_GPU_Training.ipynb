{"nbformat_minor": 2, "cells": [{"source": "# II - Single GPU Training\nIn the previous notebok we set up our pool of GPU nodes. In this notebook we are going to get one of the nodes in the pool to train a deep learning model for small number of epochs. The model and results of the training will be then loaded into blob storage for later retrieval.\n\n* [Setup](#section1)\n* [Configure job](#section2)\n* [Submit job](#section3)\n* [Delete job](#section4)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id='section1'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Setup", "cell_type": "markdown", "metadata": {}}, {"source": "Create a simple alias for Batch Shipyard", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "%alias shipyard SHIPYARD_CONFIGDIR=config python $HOME/batch-shipyard/shipyard.py %l", "outputs": [], "metadata": {"collapsed": true}}, {"source": "Check that everything is working", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section2'><\/a>\n## Configure job\nWe need to configure our job to download training and test data. The following code simply randomly selects servers to download this data from and is a byproduct of ensuring sufficient bandwidth for attendees downloading concurrently during the workshop. This random selection code and distribution of data would not be needed for typical training jobs. Typically, training data would be stored in your Azure Storage account instead.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "import random\n\nCNTK_TRAIN_DATA_FILE = 'Train_cntk_text.txt'\nCNTK_TEST_DATA_FILE = 'Test_cntk_text.txt'\nURL_FMT = 'https://{}.blob.core.windows.net/{}/{}'\n\ndef select_random_data_storage_container():\n    \"\"\"Randomly select a storage account and container for CNTK train/test data.\n    This is specific for the workshop to help distribute attendee load. This\n    function will only work on Python2\"\"\"\n    ss = random.randint(0, 4)\n    cs = random.randint(0, 4)\n    sa = '{}{}bigai'.format(ss, chr(ord('z') - ss))\n    cont = '{}{}{}'.format(cs, chr(ord('i') - cs * 2), chr(ord('j') - cs * 2))\n    return sa, cont\n\ndef create_resource_file_list():\n    sa, cont = select_random_data_storage_container()\n    ret = [{\n        'file_path': CNTK_TRAIN_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TRAIN_DATA_FILE)\n    }]\n    sa, cont = select_random_data_storage_container()\n    ret.append({\n        'file_path': CNTK_TEST_DATA_FILE,\n        'blob_source': URL_FMT.format(sa, cont, CNTK_TEST_DATA_FILE)\n    })\n    return ret", "outputs": [], "metadata": {"collapsed": true}}, {"source": "\nIn the dictonary below we define the properties of the job we wish to execute. You can see that we have specified that the image to use is the one we defined at the beginning of this notebook. Another interesting note is that we specify the gpu switch to true since we want the job to use the GPU. Finally the command is as follows:\n\n```\nsource /cntk/activate-cntk\npython /code/ConvNet_CIFAR10.py\n```\n\nWhich in essence activates the CNTK Anaconda environment then runs the **ConvNet_CIFAR10.py** script which will train and evaluate the model.\n\nIn the jobs json below, `resource_files` contains the URLs for CNTK training and test data for this script.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "TASK_ID = 'run_cifar10' # This should be changed per task\n\nJOB_ID = 'cntk-training-job'\n\nIMAGE_NAME = \"masalvar/cntkcifar\" # Custom CNTK image\n\nCOMMAND = 'bash -c \"source /cntk/activate-cntk; python -u /code/ConvNet_CIFAR10.py\"'\n\nOUTPUT_STORAGE_ALIAS = \"mystorageaccount\"\n\njobs = {\n    \"job_specifications\": [\n        {\n            \"id\": JOB_ID,\n            \"tasks\": [\n                {\n                    \"id\": TASK_ID,\n                    \"image\": IMAGE_NAME,\n                    \"remove_container_after_exit\": True,\n                    \"command\": COMMAND,\n                    \"gpu\": True,\n                    \"resource_files\": create_resource_file_list(),\n                    \"output_data\": {\n                        \"azure_storage\": [\n                            {\n                                \"storage_account_settings\": OUTPUT_STORAGE_ALIAS,\n                                \"container\": \"output\",\n                                \"source\": \"$AZ_BATCH_TASK_WORKING_DIR/Models\"\n                            },\n                        ]\n                    },\n                }\n            ],\n        }\n    ]\n}", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Write the jobs configuration to the `jobs.json` file:", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"execution_count": null, "cell_type": "code", "source": "import json\nimport os\n\ndef write_json_to_file(json_dict, filename):\n    \"\"\" Simple function to write JSON dictionaries to files\n    \"\"\"\n    with open(filename, 'w') as outfile:\n        json.dump(json_dict, outfile)\n\nwrite_json_to_file(jobs, os.path.join('config', 'jobs.json'))\nprint(json.dumps(jobs, indent=4, sort_keys=True))", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section3'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Submit job\nCheck that everything is ok with our pool before we submit our jobs\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard pool listnodes", "outputs": [], "metadata": {"collapsed": false}}, {"source": "Now that we have confirmed everything is working we can execute our job using the command below. The tail switch at the end will stream stdout from the node.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs add --tail stdout.txt", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "We can also retrieve this `stdout.txt` data independently of `--tail` above by using the `data stream` command. Note that when you delete the job all this information is also deleted.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard data stream --filespec $JOB_ID,$TASK_ID,stdout.txt", "outputs": [], "metadata": {"scrolled": false, "collapsed": false}}, {"source": "If something goes wrong you can run the following command to get the stderr output from the job.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard data stream --filespec $JOB_ID,$TASK_ID,stderr.txt", "outputs": [], "metadata": {"collapsed": false}}, {"source": "<a id='section4'><\/a>", "cell_type": "markdown", "metadata": {}}, {"source": "## Delete job", "cell_type": "markdown", "metadata": {}}, {"source": "To delete the job use the command below. Just be aware that this will get rid of all the files created by the job and tasks.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "shipyard jobs del -y --termtasks --wait", "outputs": [], "metadata": {"scrolled": true, "collapsed": false}}, {"source": "[Next notebook: Scoring](03_Scoring_model.ipynb)", "cell_type": "markdown", "metadata": {}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}